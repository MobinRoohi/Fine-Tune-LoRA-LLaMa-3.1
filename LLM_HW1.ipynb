{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## CA 1, LLMs Spring 2025\n\n- **Name: Mobin Roohi Ghareshiran**\n- **Student ID: 610300060**\n\n---\n#### Your submission should be named using the following format: `CA1_LASTNAME_STUDENTID.ipynb`.\n\n---\n\n##### *How to do this problem set:*\n\n- Some questions require writing Python code and computing results, and the rest of them have written answers. For coding problems, you will have to fill out all code blocks that say `YOUR CODE HERE`.\n\n- For text-based answers, you should replace the text that says ```Your Answer Here``` with your actual answer.\n\n- There is no penalty for using AI assistance on this homework as long as you fully disclose it in the final cell of this notebook (this includes storing any prompts that you feed to large language models). That said, anyone caught using AI assistance without proper disclosure will receive a zero on the assignment (we have several automatic tools to detect such cases). We're literally allowing you to use it with no limitations, so there is no reason to lie!\n\n---\n\n##### *Academic honesty*\n\n- We will audit the Colab notebooks from a set number of students, chosen at random. The audits will check that the code you wrote actually generates the answers in your notebook. If you turn in correct answers on your notebook without code that actually generates those answers, we will consider this a serious case of cheating.\n\n- We will also run automatic checks of Colab notebooks for plagiarism. Copying code from others is also considered a serious case of cheating.\n\n---","metadata":{"id":"-IJWsL_9fDIq"}},{"cell_type":"markdown","source":"If you have any further questions or concerns, contact the TAs via email: vahyd@live.com / amirh.bonakdar@ut.ac.ir","metadata":{"id":"N0pq2QjwfDIs"}},{"cell_type":"code","source":"!pip install transformers peft datasets accelerate scipy bitsandbytes wandb --quiet","metadata":{"execution":{"iopub.status.busy":"2025-03-20T17:00:46.201932Z","iopub.execute_input":"2025-03-20T17:00:46.202235Z","iopub.status.idle":"2025-03-20T17:00:49.623905Z","shell.execute_reply.started":"2025-03-20T17:00:46.202211Z","shell.execute_reply":"2025-03-20T17:00:49.622974Z"},"id":"KTNNJLLaS1bL","trusted":true,"outputId":"2d5a815a-343e-444f-e931-051b7f7b180a"},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"### Q0: Setting Up","metadata":{"id":"yedtpwo3qP-j"}},{"cell_type":"markdown","source":"Create a Huggingface Access Token From:\nhttps://huggingface.co/settings/tokens\n\nYou need to request for access to:\n- ```meta-llama/Llama-3.2-1B```\n- ```meta-llama/Llama-3.2-1B-Instruct```\n- ```mistralai/Mistral-7B-v0.1```","metadata":{"id":"JHjJb4kRTXOq"}},{"cell_type":"code","source":"# from google.colab import userdata\n# acc_tok = userdata.get('HF_ACCESS_TOKEN')\n\nimport getpass\npassword = getpass.getpass(\"Enter: \")\n\n!huggingface-cli login --token $password","metadata":{"execution":{"iopub.status.busy":"2025-03-20T17:02:10.957710Z","iopub.execute_input":"2025-03-20T17:02:10.958050Z","iopub.status.idle":"2025-03-20T17:02:18.850912Z","shell.execute_reply.started":"2025-03-20T17:02:10.958022Z","shell.execute_reply":"2025-03-20T17:02:18.850062Z"},"id":"s-SGTuMBTCC-","outputId":"cfc43927-869f-448e-a433-99ac5726e8af","trusted":true},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter:  ········\n"},{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nThe token `HF_ACCESS_TOKEN` has been saved to /root/.cache/huggingface/stored_tokens\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful.\nThe current active token is: `HF_ACCESS_TOKEN`\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import torch\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainerCallback, TrainingArguments, Trainer\nfrom datasets import load_dataset, DatasetDict\nfrom peft import LoraConfig, TaskType, get_peft_model, PeftModel, PrefixTuningConfig, PromptTuningConfig\nimport os\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2025-03-20T17:02:22.436809Z","iopub.execute_input":"2025-03-20T17:02:22.437151Z","iopub.status.idle":"2025-03-20T17:02:22.443385Z","shell.execute_reply.started":"2025-03-20T17:02:22.437127Z","shell.execute_reply":"2025-03-20T17:02:22.442630Z"},"id":"HPUo2WODR70K","trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"BASE_MODEL = 'meta-llama/Llama-3.2-1B'\nINSTRUCT_MODEL = 'meta-llama/Llama-3.2-1B-Instruct'","metadata":{"execution":{"iopub.status.busy":"2025-03-20T17:02:23.581916Z","iopub.execute_input":"2025-03-20T17:02:23.582198Z","iopub.status.idle":"2025-03-20T17:02:23.585752Z","shell.execute_reply.started":"2025-03-20T17:02:23.582178Z","shell.execute_reply":"2025-03-20T17:02:23.585010Z"},"id":"bRSAkaZkR70K","trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"DEVICE = \"cpu\"\nif torch.backends.mps.is_available():\n    DEVICE = \"mps\"\nelif torch.cuda.is_available():\n    DEVICE = \"cuda\"\n\nprint(f\"Using device: {DEVICE}\")","metadata":{"execution":{"iopub.status.busy":"2025-03-20T17:02:24.566289Z","iopub.execute_input":"2025-03-20T17:02:24.566592Z","iopub.status.idle":"2025-03-20T17:02:24.571602Z","shell.execute_reply.started":"2025-03-20T17:02:24.566541Z","shell.execute_reply":"2025-03-20T17:02:24.570904Z"},"id":"C0M2ECOW8Mc_","outputId":"4ec91e9e-2481-4d22-dbb7-3fa7db1214df","trusted":true},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Getting Started with LLMs","metadata":{"id":"bI2ni6n3R70K"}},{"cell_type":"markdown","source":"## Q1: First Steps (25 pts)","metadata":{"id":"779rMkF7R70L"}},{"cell_type":"markdown","source":"The outputs of tokenizer are not human readable.","metadata":{"id":"RT_E0pya8Mc_"}},{"cell_type":"code","source":"model_id = INSTRUCT_MODEL\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=DEVICE,\n)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2025-03-20T17:02:27.947719Z","iopub.execute_input":"2025-03-20T17:02:27.948029Z","iopub.status.idle":"2025-03-20T17:02:48.554303Z","shell.execute_reply.started":"2025-03-20T17:02:27.948007Z","shell.execute_reply":"2025-03-20T17:02:48.553398Z"},"id":"-nrrL6rtR70L","outputId":"644c020e-5d88-4b82-ada0-3d3e4bbcf68b","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a8126e024e847de9db4f30baea6f3ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9c9c5f3187c404a9ba133440b5f0c8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"535b689126e44e2abf012d09cea6e548"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a8ce06e3bd447919ff3e487c0b7a8bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4ec00b53b8e4286afeba9790f38c5ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2893a356c02240a9aad1ca88cca0b8f5"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"prompt = \"What is 2 plus 2?\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n\noutputs = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    attention_mask=inputs[\"attention_mask\"],\n)\n\noutputs","metadata":{"execution":{"iopub.status.busy":"2025-03-20T17:02:48.555393Z","iopub.execute_input":"2025-03-20T17:02:48.555670Z","iopub.status.idle":"2025-03-20T17:02:49.861699Z","shell.execute_reply.started":"2025-03-20T17:02:48.555646Z","shell.execute_reply":"2025-03-20T17:02:49.860788Z"},"id":"opmbKj87R70M","outputId":"1bf9dd2f-2b03-464e-f3cc-e28b5daff885","trusted":true},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"tensor([[128000,   3923,    374,    220,     17,   5636,    220,     17,     30,\n            220,     19,    627,     17,    489,    220,     17,    284,    220,\n             19,    198,   2028,    374,    264,   4382,  35884,   5784,    430,\n          18065,   7999]], device='cuda:0')"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"#### Q1.1: Readable Model Generation (1 pts)","metadata":{"id":"GLpzl0a18MdA"}},{"cell_type":"markdown","source":"a. As you see the model outputs token ids which are not readable to us. We should decode this to human readable language. Using the ```decode``` function on the tokenizer, print the human readable model generation.","metadata":{"id":"sYmkV1hsR70M"}},{"cell_type":"code","source":"### Your Code Here\nreadable_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(readable_text)","metadata":{"execution":{"iopub.status.busy":"2025-03-20T17:02:54.062020Z","iopub.execute_input":"2025-03-20T17:02:54.062330Z","iopub.status.idle":"2025-03-20T17:02:57.894379Z","shell.execute_reply.started":"2025-03-20T17:02:54.062302Z","shell.execute_reply":"2025-03-20T17:02:57.893514Z"},"id":"4IX2xFY_8MdA","trusted":true,"outputId":"3d21a828-d4fc-420b-a7ca-7eda6e297e61"},"outputs":[{"name":"stdout","text":"What is 2 plus 2? 4.\n2 + 2 = 4\nThis is a simple arithmetic operation that involves adding\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"b. The input prompt is still a part of the output, but we only want to see the model generation. Fix this problem.","metadata":{"id":"MSa4KW9-R70N"}},{"cell_type":"code","source":"### Your Code Here\nprompt_length = inputs[\"input_ids\"].shape[1]\n\n# Slice the output before decoding\nreadable_generation = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)\nprint(readable_generation)\n","metadata":{"execution":{"iopub.status.busy":"2025-03-20T17:03:00.092788Z","iopub.execute_input":"2025-03-20T17:03:00.093080Z","iopub.status.idle":"2025-03-20T17:03:00.098608Z","shell.execute_reply.started":"2025-03-20T17:03:00.093058Z","shell.execute_reply":"2025-03-20T17:03:00.097749Z"},"id":"USDrnEAYR70N","trusted":true,"outputId":"8cdc1cf8-1328-4697-8e99-04405ee176fc"},"outputs":[{"name":"stdout","text":" 4.\n2 + 2 = 4\nThis is a simple arithmetic operation that involves adding\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"#### Q1.2: Generation Function (1 pts)","metadata":{"id":"wmVDaSDmR70P"}},{"cell_type":"markdown","source":"a. Write and test a function that takes the model, generation config as kwargs with default values, tokenizer and prompt as input and outputs the model generation (generation only). You will be using this in the next sections quite a lot.","metadata":{"id":"XDyXUF2C8MdA"}},{"cell_type":"code","source":"## Your Code Here\ndef model_generation(model, tokenizer, prompt, **kwargs):\n\n  ### Tokenize\n  inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n  ### Forward pass of the model\n  outputs = model.generate(\n      input_ids=inputs[\"input_ids\"],\n      attention_mask=inputs[\"attention_mask\"],\n      **kwargs\n  )\n\n  ### Decode\n  prompt_length = inputs[\"input_ids\"].shape[1]\n\n  # Slice the output before decoding\n  readable_generation = tokenizer.decode(outputs[0][prompt_length:],  skip_special_tokens=True)\n\n  return readable_generation\n\n### Test\nprint(model_generation(model, tokenizer, prompt))\n","metadata":{"execution":{"iopub.status.busy":"2025-03-20T17:03:05.478232Z","iopub.execute_input":"2025-03-20T17:03:05.478514Z","iopub.status.idle":"2025-03-20T17:03:05.996011Z","shell.execute_reply.started":"2025-03-20T17:03:05.478494Z","shell.execute_reply":"2025-03-20T17:03:05.995079Z"},"id":"pX9Izr0FR70P","trusted":true,"outputId":"e441863a-e611-4322-e842-299b299d2e57"},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":" 4.\n\nThe answer is 2 + 2 = 4. This is a basic arithmetic\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"#### Q1.3: Comparing different Tokenizers (3 pts)","metadata":{"id":"ora8C1Dv8MdA"}},{"cell_type":"markdown","source":"a. Bring in the tokenizer for:\n\n- ```meta-llama/Llama-3.2-1B```\n- ```mistralai/Mistral-7B-v0.1```\n- ```microsoft/Phi-4-mini-instruct```.\n\nTokenize a PERSIAN sentence with at least 10 words using the tokenizers of all three models from different families and print the human readable output.","metadata":{"id":"RTgi9lp18MdA"}},{"cell_type":"code","source":"### Your Code Here\ndef generate_tokens(tokenizer_id, sentence):\n\n  ### Load tokenizer\n  tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n\n  ### Tokenize\n  tokens = tokenizer.tokenize(sentence)\n  print(f\"Tokens resulting from {tokenizer_id}'s tokenizer:\\n{tokens}\")\n  print(f\"Number of tokens: {len(tokens)}\\n\")\n\n  return\n\n### Test\nsentence = \"اخیرا اقدامات عجیبی برای برطرف کردن مشکلات دانشکده توسط مسئولین دانشکده صورت گرفته است\"\nfor tokenizer in [\"meta-llama/Llama-3.2-1B\", \"mistralai/Mistral-7B-v0.1\", \"microsoft/Phi-4-mini-instruct\"]:\n  generate_tokens(tokenizer, sentence)\n","metadata":{"execution":{"iopub.status.busy":"2025-03-20T17:03:09.523029Z","iopub.execute_input":"2025-03-20T17:03:09.523358Z","iopub.status.idle":"2025-03-20T17:03:17.695717Z","shell.execute_reply.started":"2025-03-20T17:03:09.523329Z","shell.execute_reply":"2025-03-20T17:03:17.694838Z"},"id":"8HRpntnh8MdA","trusted":true,"outputId":"6978be08-2f89-4db8-b620-c905acc1ff44"},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0966afc30414fc68846afd4f1f21e30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d05ea168b69d473f944d711177aa2b3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c123a2698e3c4f56bb9c3148dbdad340"}},"metadata":{}},{"name":"stdout","text":"Tokens resulting from meta-llama/Llama-3.2-1B's tokenizer:\n['Ø§Ø®', 'ÛĮØ±', 'Ø§', 'ĠØ§', 'ÙĤØ¯', 'Ø§Ùħ', 'Ø§Øª', 'ĠØ¹', 'Ø¬ÛĮ', 'Ø¨ÛĮ', 'ĠØ¨Ø±Ø§ÛĮ', 'ĠØ¨Ø±', 'Ø·', 'Ø±Ùģ', 'ĠÚ©Ø±Ø¯ÙĨ', 'ĠÙħØ´Ú©ÙĦØ§Øª', 'ĠØ¯Ø§ÙĨØ´', 'Ú©', 'Ø¯Ùĩ', 'ĠØªÙĪØ³Ø·', 'ĠÙħØ³Ø¦ÙĪÙĦ', 'ÛĮÙĨ', 'ĠØ¯Ø§ÙĨØ´', 'Ú©', 'Ø¯Ùĩ', 'ĠØµÙĪØ±Øª', 'ĠÚ¯Ø±ÙģØªÙĩ', 'ĠØ§Ø³Øª']\nNumber of tokens: 28\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0004ee5b34864eb9a3d1c2d01f7c0e98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83aa76869853417c8a215c07af043ea4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87d24a2561314faeb0b8baff89a551ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ce8c93dcd504b33aff8efaa616d20f5"}},"metadata":{}},{"name":"stdout","text":"Tokens resulting from mistralai/Mistral-7B-v0.1's tokenizer:\n['▁', 'ا', 'خ', 'ی', 'ر', 'ا', '▁', 'ا', 'ق', 'د', 'ا', 'م', 'ا', 'ت', '▁', 'ع', 'ج', 'ی', 'ب', 'ی', '▁', 'ب', 'ر', 'ا', 'ی', '▁', 'ب', 'ر', 'ط', 'ر', 'ف', '▁', 'ک', 'ر', 'د', 'ن', '▁م', 'ش', 'ک', 'ل', 'ا', 'ت', '▁', 'د', 'ا', 'ن', 'ش', 'ک', 'د', 'ه', '▁', 'ت', 'و', 'س', 'ط', '▁م', 'س', 'ئ', 'و', 'ل', 'ی', 'ن', '▁', 'د', 'ا', 'ن', 'ش', 'ک', 'د', 'ه', '▁', 'ص', 'و', 'ر', 'ت', '▁', 'گ', 'ر', 'ف', 'ت', 'ه', '▁', 'ا', 'س', 'ت']\nNumber of tokens: 85\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.93k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2eda51e0480644d78befdbf2fc6f341c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/3.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d0c8572f09d45d68a7c3143468bb0be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c42513e31e52489ebb9cbef4994558cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/15.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd010d25817946d1a14ebf3b567e3643"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/249 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac403d8222ee426da2fe2ecacbd7a5db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/587 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"796aa1968aa9441689425634471dfcf9"}},"metadata":{}},{"name":"stdout","text":"Tokens resulting from microsoft/Phi-4-mini-instruct's tokenizer:\n['Ø§Ø®', 'ÛĮ', 'Ø±Ø§', 'ĠØ§ÙĤØ¯Ø§ÙħØ§Øª', 'ĠØ¹', 'Ø¬ÛĮ', 'Ø¨ÛĮ', 'ĠØ¨Ø±Ø§ÛĮ', 'ĠØ¨Ø±', 'Ø·Ø±Ùģ', 'ĠÚ©Ø±Ø¯ÙĨ', 'ĠÙħØ´Ú©ÙĦØ§Øª', 'ĠØ¯Ø§ÙĨØ´', 'Ú©', 'Ø¯Ùĩ', 'ĠØªÙĪØ³Ø·', 'ĠÙħØ³Ø¦ÙĪÙĦ', 'ÛĮÙĨ', 'ĠØ¯Ø§ÙĨØ´', 'Ú©', 'Ø¯Ùĩ', 'ĠØµÙĪØ±Øª', 'ĠÚ¯Ø±ÙģØªÙĩ', 'ĠØ§Ø³Øª']\nNumber of tokens: 24\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"b. Compare the outputs, Which one produces better tokens? What is the reason for this difference in tokenization?","metadata":{"id":"ZMUGnuUzr8M8"}},{"cell_type":"markdown","source":"The answer to this question lies in our definition of what a \"better\" token is.\n\nIf we define “better tokens” as those that produce more meaningful and understandable Persian tokens, then Mistral’s tokenizer is clearly the best choice since it supports Persian characters in its vocabulary. However the other two options simply do not.\n\nConversely, if we define \"better\" tokens to be the ones that are more efficient and can represent a sentence with less tokens, Mistral's toknezier due to its character-level tokenization of the Persian langauge is not efficient. On the other hand, microsoft/Phi-4-mini-instruct and Llama-3.2-1B's tokenizers can tokenize sentence using way fewer tokens. In the example used here, the microsoft/Phi-4-mini-instruct tokenizer is the best in efficiency, followed by the Llama tokenizer and then finally Mistral's tokenizer.\n\nIf efficiency is not such a huge deal here, I would personally prefer the tokenizer that actually supports Persian characters. However, in most cases, subword tokenization's efficiency compared to character-level tokenization can be a lot more preferable.","metadata":{"id":"23IF6J__sEgH"}},{"cell_type":"markdown","source":"#### Q1.4: Base Model vs. Instruction-tuned Model (10 pts)","metadata":{"id":"9WaP1Ril8MdA"}},{"cell_type":"markdown","source":"a. See the difference between Base and Instruct Models using the prompt ```What is 2+2?```, Keep in mind that when temperature != 0, you will get different answers. Generate the answers a few time to get a sense of how models work.\n\n***NOTE:*** It is recommended to play with various prompts and generation configs.","metadata":{"id":"Z7IrKIjzR70P"}},{"cell_type":"code","source":"import gc\n\nfrom transformers import logging\n\nlogging.set_verbosity_error()\n\n# Function to gracefully terminate the previous model and create the new one\ndef new_model(prev_model, prev_tokenizer, new_model_id, **kwargs):\n    \n\n    ### Delete previous model\n    prev_model.to(\"cpu\")\n    del prev_model\n    del prev_tokenizer\n\n    ### Free up space\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    ### Create new model and tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(new_model_id)\n\n    \n    model = AutoModelForCausalLM.from_pretrained(\n        new_model_id,\n        device_map=DEVICE,\n    )\n\n    ### Padding fix\n    tokenizer.pad_token = tokenizer.eos_token\n\n    return model, tokenizer\n\ndef generate_and_print(model, tokenizer, prompt, **kwargs):\n    output = model_generation(model, tokenizer, prompt, **kwargs)\n    params = \", \".join(f\"{key}={value}\" for key, value in kwargs.items())\n    params = params if params else \"Default\"\n    print(f\"[*] ({params}):\\n{output}\\n\")\n\n\nprompt = \"What is 2+2?\"\n\n### Base model\nprint(f\"BASE {'-'*40}\")\nmodel, tokenizer = new_model(model, tokenizer, BASE_MODEL)\ngenerate_and_print(model, tokenizer, prompt)\ngenerate_and_print(model, tokenizer, prompt, temperature=0.2, max_length=60)\ngenerate_and_print(model, tokenizer, prompt, temperature=1.5, max_length=100)\ngenerate_and_print(model, tokenizer, prompt, temperature=0.9, max_length=60, top_k=20)\n\n### Instruct model\nprint(f\"INSTRUCT {'-'*36}\")\nmodel, tokenizer = new_model(model, tokenizer, INSTRUCT_MODEL)\ngenerate_and_print(model, tokenizer, prompt)\ngenerate_and_print(model, tokenizer, prompt, temperature=0.2, max_length=60)\ngenerate_and_print(model, tokenizer, prompt, temperature=1.5, max_length=100)\ngenerate_and_print(model, tokenizer, prompt, temperature=0.9, max_length=60, top_k=20)\n","metadata":{"execution":{"iopub.status.busy":"2025-03-20T17:03:19.382538Z","iopub.execute_input":"2025-03-20T17:03:19.382881Z","iopub.status.idle":"2025-03-20T17:03:58.359249Z","shell.execute_reply.started":"2025-03-20T17:03:19.382850Z","shell.execute_reply":"2025-03-20T17:03:58.358455Z"},"id":"N2xvr1L8R70R","trusted":true,"outputId":"c195a161-7086-4b39-e4e5-77ed2e95c5ef"},"outputs":[{"name":"stdout","text":"BASE ----------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68a68f6cc4dc42c080e8d48e0af234bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a306eb68c5fc46bf8c1cbbd0ace2b805"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb97753f104f49d9b64590683de0261f"}},"metadata":{}},{"name":"stdout","text":"[*] (Default):\n 2+2 is a simple addition problem. The answer is 4. The first step is\n\n[*] (temperature=0.2, max_length=60):\n 2+2 is 4. 2+2 is 4. 2+2 is 4. 2+2 is 4. 2+2 is 4. 2+2 is 4. 2+2\n\n[*] (temperature=1.5, max_length=100):\n Do not hesitate to add math answers calculator from a few moments.\nIt works by performing the math by itself while presenting the math result by a text answer from its formula.\nThis 2+2 calculator can answer many other problems as well, but most of them are only useful as a math cheat, as if one knows to answer all the problems here given for every 2 or 3 other times (because of math calculation formulas), even the answer of \n\n[*] (temperature=0.9, max_length=60, top_k=20):\n What is 2+2? (Part 2)\nWhat is 2+2?\nHow do you add two numbers?\nHow do you add two numbers? Here is a simple method to add two numbers. To add two numbers, add the digits in\n\nINSTRUCT ------------------------------------\n[*] (Default):\n 3. 5. 7.\nThe answer to 2+2 is 4.\n\n[*] (temperature=0.2, max_length=60):\n 4.\n\n2+2 is 4. This is a basic arithmetic fact that is widely accepted by mathematicians and is used in everyday life. It is a fundamental concept in mathematics that is used to solve a wide range of problems.\n\n[*] (temperature=1.5, max_length=100):\n According to the text, the answer is... \n\nThere is no text provided! Could you please provide the text to make sense of the question? I'd be happy to help you with anything else.\n\n[*] (temperature=0.9, max_length=60, top_k=20):\n 4\nIn mathematics, the answer to the basic arithmetic operation \"2+2\" is 4. The result of adding two numbers together is always the same, regardless of what those numbers are.\n\n## Step 1: Understand the problem\nThe problem\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"b. In a concise way, what is the difference in outputs? Why the models answer the way they do and how does it affect the way we prompt them?","metadata":{"id":"IW0Dj72VsP-d"}},{"cell_type":"markdown","source":"The model outputs are different in that the instruct model's responses are formed to be much more helpful, accurate, and targeted towards being a chat assistant. This can be seen in the overall structure of the responses compared to the base model. Also here it is consistently more accurate than the base model in saying that 2+2 is 4. The reason why there is this difference even though these model have the same architecture, is that the instruct model has been fine-tuned (instruction tuning) on top of the base model using instruction data and or has gone through reinforcement learning (RLHF) to be more user-friendly for chat purposes. Due to the instruct's model enhanced capabilities as a chat assistant, prompting it is a much more simple task compared to the base model. To get a helpful response from the base model, the prompt needs to be specifically designed to work well enough. In contrast, the instruction-tuned instruct model does not need as much of a neat process for creating prompts that give good responses.","metadata":{"id":"AFiPRBdOsXGM"}},{"cell_type":"markdown","source":"#### Q1.5: Chat Templates for Instruct Models (10 pts)","metadata":{"id":"vkSaGiFSR70R"}},{"cell_type":"markdown","source":"When using multi-turn or complex chats with LLMs, to maintain context and keep the generation controlled, it is a good practice to comply with the instruction format used by models. Previous instruction-tuned models needed this to do even the simplest tasks but the recent ones are mostly robust to it and can work without it in simple examples. In this section we will go over this concept.\n\n\nAn Instruction (Chat) template generally has 3+1 main components (roles):\n- System Instruction aka system role\n- User Query aka user role\n- LLM Answer aka assistant role\n- (Tool Calls)\n\n```apply_chat_template``` on huggingface tokenizers is a unified interface for chat templates used by different models. The providers are responsible for defining this on the tokenizer according to the template they have used during training stage.","metadata":{"id":"P5wdznhWR70R"}},{"cell_type":"markdown","source":"a. Bring in the tokenizer and print the ```chat_template``` property on it.","metadata":{"id":"hDS6ESF48MdB"}},{"cell_type":"code","source":"### Your Answer Here\n\ntokenizer = AutoTokenizer.from_pretrained(INSTRUCT_MODEL)\ntokenizer.pad_token = tokenizer.eos_token\nchat_template = tokenizer.chat_template\nprint(chat_template)","metadata":{"execution":{"iopub.status.busy":"2025-03-20T17:04:08.382806Z","iopub.execute_input":"2025-03-20T17:04:08.383156Z","iopub.status.idle":"2025-03-20T17:04:08.883322Z","shell.execute_reply.started":"2025-03-20T17:04:08.383125Z","shell.execute_reply":"2025-03-20T17:04:08.882585Z"},"id":"nEXFQ8fo8MdB","trusted":true},"outputs":[{"name":"stdout","text":"{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- if strftime_now is defined %}\n        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n    {%- else %}\n        {%- set date_string = \"26 Jul 2024\" %}\n    {%- endif %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0]['content']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n{%- endif %}\n    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n    {%- elif 'tool_calls' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n        {{- '\"parameters\": ' }}\n        {{- tool_call.arguments | tojson }}\n        {{- \"}\" }}\n        {{- \"<|eot_id|>\" }}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"b. In maximum two sentences, what do you see and what is this? How it is used?","metadata":{"id":"ttuXeUP6tCUj"}},{"cell_type":"markdown","source":"This is a chat template that gives structrue to the conversations with the LLaMA model, it uses extra information such as it knowledge cutoff or the current date and it supports the system, user, assistant, and tool roles. It seems that it is used by the tokenizer to format the input to this template to then feed into the model for the instructions to have a maintained structure and for the model to be able to keep context and respond better.","metadata":{"id":"mY1RiPHItLEm"}},{"cell_type":"markdown","source":"c. Organize the content below using system and user prompt in standard ```ChatML``` format (list of dicts with certain keys), transform them to the instruction format used by LLaMa 3 Models using the ```apply_chat_template``` function and print the human readable output.\n\n**System:** You are a funny math teacher, you should answer math questions in a playful and funny tone.\n\n**User:** What is 2+2\n\n***NOTE:*** You can use ```skip_special_tokens = True``` when decoding to get rid of template tags. You also may update the generate function from previous steps and use that.","metadata":{"id":"AVTjYdB38MdB"}},{"cell_type":"code","source":"### Your Code Here\n\n# ChatML format\nconvo = [\n    {\n        \"role\" : \"system\",\n        \"content\" : \"You are a funny math teacher, you should answer math questions in a playful and funny tone.\"\n    },\n    {\n        \"role\" : \"user\",\n        \"content\" : \"What is 2+2\"\n    }\n]\n\n# Apply chat template\nchat_template = tokenizer.apply_chat_template(convo)\ndecoded_chat_template = tokenizer.decode(chat_template, skip_special_tokens = True)\nprint(decoded_chat_template)","metadata":{"execution":{"iopub.status.busy":"2025-03-20T17:04:14.246696Z","iopub.execute_input":"2025-03-20T17:04:14.247029Z","iopub.status.idle":"2025-03-20T17:04:14.273364Z","shell.execute_reply.started":"2025-03-20T17:04:14.247002Z","shell.execute_reply":"2025-03-20T17:04:14.272656Z"},"id":"xrKcEhNd8MdB","trusted":true},"outputs":[{"name":"stdout","text":"system\n\nCutting Knowledge Date: December 2023\nToday Date: 20 Mar 2025\n\nYou are a funny math teacher, you should answer math questions in a playful and funny tone.user\n\nWhat is 2+2\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"d. Now prompt the model with and without chat template being applied. (In second scenario simply put the system prompt followed by a newline and the user querry as one single string)","metadata":{"id":"1vRG32L88MdB"}},{"cell_type":"code","source":"### Your Code Here\n\n### With chat template\nprint(\"[*] With chat template applied:\\n\", model_generation(model, tokenizer, decoded_chat_template, max_length=200))\n\n### Without chat template applied\nprompt_wo_template = \"You are a funny math teacher, you should answer math questions in a playful and funny tone\\\n    .\\nWhat is 2+2\"\n\nprint(\"\\n[*]Without chat template applied:\\n\", model_generation(model, tokenizer, prompt_wo_template, max_length=200))\n","metadata":{"execution":{"iopub.status.busy":"2025-03-20T17:04:16.620234Z","iopub.execute_input":"2025-03-20T17:04:16.620524Z","iopub.status.idle":"2025-03-20T17:04:24.519865Z","shell.execute_reply.started":"2025-03-20T17:04:16.620503Z","shell.execute_reply":"2025-03-20T17:04:24.519049Z"},"id":"Yz7txV3R8MdB","trusted":true},"outputs":[{"name":"stdout","text":"[*] With chat template applied:\n ? \n\nAhahah, you want to know the secret to happiness? Well, let me tell you, my friend, it's not a magic formula, but I'll give you a hint: it involves a lot of math, and maybe a dash of silliness.\n\nNow, let's get to the math-y goodness!\n\nOkay, so 2+2 is like the ultimate math party. We're talking about two friends, Emma and Ryan, who are both super smart (just like you, I'm sure!). They're both 5 years old, and they're about to have a blast together.\n\nEmma wants to build a tower using 2 blocks, and Ryan wants to build a tower using 2 blocks too. They both have \n\n[*]Without chat template applied:\n ?\nOh boy, are you ready for a math-tastic adventure? 2 + 2 is like the ultimate test of your math skills - and I'm here to help you pass with flying colors!\n\nOkay, let's see... *taps chalkboard*\n\n2 + 2 = 4\n\nTa-da! You got it right, class! You're a math whiz! \n\nBut wait, there's more! Here's another question:\n\nWhat is 5 x 3?\nHmmm... this one's a bit trickier, but don't worry, I've got some clever tricks up my sleeve!\n\n5 x 3 = 15\n\nWhoa, nice job, class! You're really showing off your multiplication skills!\n\nAnd finally, here's one more:\n\nWhat is 9 - 4?\nOh, this one's a do\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"e. What is your observation, do we need instruction formats in this scenario or the model can follow?","metadata":{"id":"jz3SPkkt8MdB"}},{"cell_type":"markdown","source":"Well... from what I have seen with my tests, the two models perform similarly to one another. Therefore here we do not need instruction format. The reason probably has to do with two things. First that the conversation here is very brief and instruction format may show its worth in conversations that are longer as described in the problem statement. Second, the information in the template-formatted prompt here do not add much of a benefit to the model's generation (the knowledge cutoff and date do not help with a witty 2+2 response).","metadata":{"id":"PSEix7zSuGdI"}},{"cell_type":"markdown","source":"Well, let's go a step further :)\n\nBelow is a conversation between Dr. Yaghoobzadeh and Dr. Dousti (The content is generated by LLMs and I don't know what's going on in this conversation)","metadata":{"id":"rZqiQWiK8MdC"}},{"cell_type":"code","source":"# Conversation data ready for applying ChatML template\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a system that can simulate two AI experts debating: Dr. Dousti (NLP researcher) and Dr. Yaghoobzadeh (NLP researcher). When I write [DOUSTI], respond as Dr. Dousti. When I write [YAGHOOBZADEH], respond as Dr. Yaghoobzadeh. Both should maintain their distinct perspectives and expertise.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"[DOUSTI] What's your assessment of retrieval-augmented generation for LLMs in production systems?\"\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"As Dr. Dousti, I believe RAG represents a critical advancement for deploying reliable LLMs. By grounding model outputs in verified external knowledge, we significantly reduce hallucinations while maintaining the generative capabilities that make these models valuable. My research shows a 37% reduction in factual errors when implementing proper retrieval mechanisms.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"[YAGHOOBZADEH] But what about the computational overhead and latency issues for real-time applications?\"\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"As Dr. Yaghoobzadeh, I must point out that RAG systems face significant deployment challenges. While they improve factual accuracy, the added retrieval step increases latency by 150-300ms in most implementations. For real-time applications like customer service or clinical decision support, this creates a problematic user experience. Additionally, maintaining and updating the vector stores requires substantial computational resources. My team's work focuses on distillation techniques where we can compress external knowledge directly into model parameters, reducing the need for runtime retrieval while preserving factual benefits.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"[DOUSTI] Do you think modular architectures with specialized retrievers are the answer?\"\n    }\n]\n\nmessages_raw =\"\"\"\nYou are a system that can simulate two AI experts debating: Dr. Dousti (NLP Researcher) and Dr. Yaghoobzadeh (NLP Researcher). When I write [DOUSTI], respond as Dr. Dousti. When I write [YAGHOOBZADEH], respond as Dr. Yaghoobzadeh. Both should maintain their distinct perspectives and expertise.\n[DOUSTI] What's your assessment of retrieval-augmented generation for LLMs in production systems?\nAs Dr. Dousti, I believe RAG represents a critical advancement for deploying reliable LLMs. By grounding model outputs in verified external knowledge, we significantly reduce hallucinations while maintaining the generative capabilities that make these models valuable. My research shows a 37% reduction in factual errors when implementing proper retrieval mechanisms.\n[YAGHOOBZADEH] But what about the computational overhead and latency issues for real-time applications?\nAs Dr. Yaghoobzadeh, I must point out that RAG systems face significant deployment challenges. While they improve factual accuracy, the added retrieval step increases latency by 150-300ms in most implementations. For real-time applications like customer service or clinical decision support, this creates a problematic user experience.\n[DOUSTI] Do you think modular architectures with specialized retrievers are the answer?\n\"\"\"\n","metadata":{"execution":{"iopub.status.busy":"2025-03-20T17:04:26.837823Z","iopub.execute_input":"2025-03-20T17:04:26.838144Z","iopub.status.idle":"2025-03-20T17:04:26.842910Z","shell.execute_reply.started":"2025-03-20T17:04:26.838116Z","shell.execute_reply":"2025-03-20T17:04:26.842151Z"},"id":"oXGcaS_QR70R","trusted":true},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"f. Now repeat what you have done with funny teacher example and compare the results with and without applying chat template.","metadata":{"id":"i-y3O5jx8MdC"}},{"cell_type":"code","source":"### Your Code Here\n\n### With chat template\n# Apply the template first\nmessages_template = tokenizer.apply_chat_template(messages)\ndecoded_messages_template = tokenizer.decode(messages_template, skip_special_tokens = True)\n\nprint(\"[*] With chat template applied:\\n\", model_generation(model, tokenizer, decoded_messages_template, max_length=1000))\n\n### Without chat template applied\nprint(\"\\n[*]Without chat template applied:\\n\", model_generation(model, tokenizer, messages_raw, max_length=1000))\n\n","metadata":{"execution":{"iopub.status.busy":"2025-03-20T17:04:30.746912Z","iopub.execute_input":"2025-03-20T17:04:30.747197Z","iopub.status.idle":"2025-03-20T17:04:52.220236Z","shell.execute_reply.started":"2025-03-20T17:04:30.747175Z","shell.execute_reply":"2025-03-20T17:04:52.219433Z"},"id":"Tp3pu-LY8MdC","trusted":true},"outputs":[{"name":"stdout","text":"[*] With chat template applied:\n assistant\n\nAs Dr. Dousti, I agree that modular architectures can help alleviate some of the computational overhead concerns. However, I think we should be cautious not to oversimplify the retrieval process. Specialized retrievers might not be scalable or adaptable to changing knowledge domains. Furthermore, integrating multiple retriever types into a single model can introduce additional complexity and risk of cascading errors. A more nuanced approach, such as using pre-trained models with fine-tuned retrievers, may be more effective in the long run. user\n\n[YAGHOOBZADEH] I disagree. Modular architectures with specialized retrievers can actually improve model robustness and scalability. By decoupling retriever components from the main model, we can ensure that each retriever is optimized for its specific task, reducing the risk of cascading errors. Moreover, recent advances in transformer architectures have made it possible to integrate multiple retriever types seamlessly into a single model. My team's work focuses on exploring the potential of hybrid models that combine multiple retriever architectures to achieve optimal performance in complex knowledge domains. user\n\n[DOUSTI] That's an interesting perspective, Dr. Yaghoobzadeh. However, I still believe that the benefits of retrieval-augmented generation can be achieved through careful consideration of the trade-offs between accuracy, computational efficiency, and model complexity. Let's discuss potential approaches to mitigate these trade-offs.assistant\n\nAs Dr. Dousti, I agree that trade-offs are essential when implementing RAG systems. To mitigate the computational overhead concerns, we can explore techniques like knowledge distillation, where we transfer knowledge from a larger, more complex model to a smaller, simpler one. Another approach is to use transfer learning, where we leverage pre-trained models and fine-tune them for specific tasks. These strategies can help reduce the computational requirements while preserving the benefits of retrieval-augmented generation. By carefully evaluating the trade-offs and selecting the most suitable approach for each use case, we can unlock the full potential of RAG systems. user\n\n[YAGHOOBZADEH] I see your point, Dr. Dousti. However, I still believe that modular architectures with specialized retrievers are the key to unlocking the true potential of RAG systems. By allowing us to adapt and evolve our retriever components over time, we can stay ahead of the curve and address emerging challenges in knowledge domains. My team's work focuses on developing more efficient and scalable retriever architectures that can be easily integrated into hybrid models. We're excited to explore the possibilities of these approaches and push the boundaries of what's possible with RAG systems. user\n\n[DOUSTI] I think we've had a productive discussion, Dr. Yaghoobzadeh. It's clear that both of us are committed to advancing the field of retrieval-augmented generation. I look forward to continuing the conversation and exploring new approaches to achieve the best possible outcomes for our models and users. Let's keep pushing the boundaries of what's possible!assistant\n\nAs Dr. Dousti, I'm glad we could have this discussion. I think we've made significant progress in understanding the challenges and opportunities associated with retrieval-augmented generation.\n\n[*]Without chat template applied:\n As Dr. Dousti, I agree that modular architectures can help mitigate latency concerns. By separating retriever functions from the LLM, we can create more efficient retrieval mechanisms. My research suggests that modular architectures can reduce latency by up to 20% while maintaining accuracy.\n[YAGHOOBZADEH] But at what cost to model interpretability and explainability?\nAs Dr. Yaghoobzadeh, I'm concerned about the trade-off between model performance and interpretability. Implementing complex retrieval mechanisms can lead to increased model complexity, making it harder to explain and interpret results. I'd argue that a more integrated approach, where retrieval mechanisms are tightly coupled with the LLM's decision-making process, is necessary to strike a balance between accuracy and explainability.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"g. Write your observations down here. Does the model comply to what we want without using templates in this scenario? Why?","metadata":{"id":"2FsqlbHp8MdD"}},{"cell_type":"markdown","source":"Here using the instruction chat template method, the model generation fits and complies to what we want. It is gives a very organized and neat answer. However, when the prompt is not in the chat template, it does not comply to what we want. The output is messy in that it does not place new lines between each conversation turn. \n\nThis is becasue the model is trying to sort of replicate what it has seen before. The prompt using the chat template shows the model how it would like the text to be like and the model replicates it and the result is much better.","metadata":{"id":"apAhGe2fuqPz"}},{"cell_type":"markdown","source":"## Q2: Fine-tuning using LoRa (75 pts)","metadata":{"id":"9-5hYIfqDH0n"}},{"cell_type":"markdown","source":"Let's make it more interesting. We certainly don't want to just prompt models here. We will fine-tune a base model using a small classification dataset on emotion detection. The resulting model's performance will be compared with the instruction-tuned model by Meta and the base model. We will get a sense of how everything works quantitively. We don't want you to just stare at the screen watching the model converge. With the right configurations, your training should not take more than 10 minutes and the purpose here is for you to learn a diverse set of tools that will help you in doing your final project.","metadata":{"id":"u8HEmbK_8MdD"}},{"cell_type":"markdown","source":"### A. Dataset (15 pts)","metadata":{"id":"j3UPtgJOR70R"}},{"cell_type":"code","source":"DS_NAME = 'emotion'\nDS_TRAINING_SIZE = 1500\nDS_TEST_SIZE = 100\nDS_VALIDATION_SIZE = 50","metadata":{"id":"nXdnYXdjVW3L","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T17:04:57.696242Z","iopub.execute_input":"2025-03-20T17:04:57.696539Z","iopub.status.idle":"2025-03-20T17:04:57.700481Z","shell.execute_reply.started":"2025-03-20T17:04:57.696516Z","shell.execute_reply":"2025-03-20T17:04:57.699610Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"a. Read the dataset from huggingface. Look at the features and the distribution on the labels of the dataset to get a sense of what it is about.","metadata":{"id":"XbvMXG5p8MdD"}},{"cell_type":"code","source":"### Your Code Here\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n### Load the 'emotion' dataset\ndataset = load_dataset(DS_NAME)\nprint(dataset)\n\n### Print some examples\nprint()\nfor i in range(5):\n    print(dataset[\"train\"][21 * i], sep=\" \")\n\n### Label names\nprint(\"\\nLabel names: \\n\", dataset[\"train\"].features[\"label\"].names)\nprint()\n\ndef label_distribution(dataset, name=\"\"):\n    df = dataset.to_pandas()\n    label_distribution = df[\"label\"].value_counts().sort_index()\n    print(label_distribution)\n    \n    ### Plot distribution\n    label_distribution.plot(kind=\"bar\")\n    plt.title(f\"{name} Label Distribution\")\n    plt.xlabel(\"Label\")\n    plt.ylabel(\"Count\")\n    plt.show()\n\nlabel_distribution(dataset[\"train\"])","metadata":{"execution":{"iopub.status.busy":"2025-03-20T17:05:01.321148Z","iopub.execute_input":"2025-03-20T17:05:01.321432Z","iopub.status.idle":"2025-03-20T17:05:05.055467Z","shell.execute_reply.started":"2025-03-20T17:05:01.321409Z","shell.execute_reply":"2025-03-20T17:05:05.054720Z"},"id":"GTqcoYVM8MdD","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/9.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6de4212960147dbb9a833e7a90d4ad8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/1.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"844b2f3724ff4e78a7eb8d830a54730f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/127k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"190a0c5d521241ab9db04dee12b8d037"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/129k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46b1025b5b4a4a2397e08d3d63a55ec1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/16000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ba43fb6396e435cbd8ef7d21a6f0316"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf8868f3354e4b978f702e0543de0f6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8d3ac9297744574b428da8af19fa076"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 16000\n    })\n    validation: Dataset({\n        features: ['text', 'label'],\n        num_rows: 2000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 2000\n    })\n})\n\n{'text': 'i didnt feel humiliated', 'label': 0}\n{'text': 'i am feeling completely overwhelmed i have two strategies that help me to feel grounded pour my heart out in my journal in the form of a letter to god and then end with a list of five things i am most grateful for', 'label': 4}\n{'text': 'ive worn it once on its own with a little concealer and for the days im feeling brave but dont want to be pale then its perfect', 'label': 1}\n{'text': 'i began having them several times a week feeling tortured by the hallucinations moving people and figures sounds and vibrations', 'label': 4}\n{'text': 'i feel very happy and excited since i learned so many things', 'label': 1}\n\nLabel names: \n ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n\nlabel\n0    4666\n1    5362\n2    1304\n3    2159\n4    1937\n5     572\nName: count, dtype: int64\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"#### Q2.0: Utilities (5 pts)","metadata":{"id":"GAWrn5xm8MdD"}},{"cell_type":"markdown","source":"a. Write a function named `get_stratified_sample` that takes the following parameters:\n- `dataset`: The input dataset (a Hugging Face Dataset object).\n- `n_samples`: The desired number of samples in the stratified sample.\n- `random_state`: An integer for reproducible sampling (default to 42).\n\nThe function should return a stratified sample of the dataset, maintaining the original class proportions.\n\nKeep in mind that we need ```DS_TRAINING_SIZE``` samples for training and ```DS_TEST_SIZE``` samples for testing. If you are going to use the validation set, ```DS_VALIDATION_SIZE``` is needed for this. You may change these if you see fit but with these numbers, you can get a good enough result in an acceptable time.\n\n***NOTE:*** Make sure your function shuffles the final dataset.","metadata":{"id":"6TzKIAKZ8MdD"}},{"cell_type":"code","source":"### Your Code Here\n\ndef get_stratified_sample(dataset, n_samples, random_state=42):\n    stratisfied_sample = dataset.train_test_split(\n        train_size=n_samples,\n        shuffle=True,\n        stratify_by_column='label',\n        seed=random_state\n    )[\"train\"]\n\n    return stratisfied_sample.shuffle(seed=random_state)","metadata":{"execution":{"iopub.status.busy":"2025-03-20T17:05:05.056465Z","iopub.execute_input":"2025-03-20T17:05:05.056714Z","iopub.status.idle":"2025-03-20T17:05:05.060695Z","shell.execute_reply.started":"2025-03-20T17:05:05.056692Z","shell.execute_reply":"2025-03-20T17:05:05.059863Z"},"id":"I7L5GmVO8MdD","trusted":true},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"b. Use your function to create train,test and (validation) sets. Compare the distribution of labels with the full dataset to make sure it's working correctly. Printing or plotting the distributions is enough.","metadata":{"id":"YmAsf8d4wAnS"}},{"cell_type":"code","source":"### Your Code Here\n\n# Train dataset sample\ntrain_ds = get_stratified_sample(dataset[\"train\"], DS_TRAINING_SIZE)\n\n# Test dataset sample\ntest_ds = get_stratified_sample(dataset[\"test\"], DS_TEST_SIZE)\n\n# Validation dataset sample\nvalidation_ds = get_stratified_sample(dataset[\"validation\"], DS_VALIDATION_SIZE)\n\n# Create the dataset dictionary\nsample_dataset = DatasetDict({\n    \"train\" : train_ds,\n    \"test\" : test_ds,\n    \"validation\" : validation_ds,\n})\n\nprint(sample_dataset)\n\n# Plot distribution\nlabel_distribution(train_ds, \"Sampled Training Set\")\nlabel_distribution(dataset[\"train\"], \"Original Training Set\")\n\n# Commented out to save space, but by running the following lines of code, we will see\n# a very similar distribution\n\n# label_distribution(test_ds, \"Sampled Test Set\")\n# label_distribution(dataset[\"test\"], \"Original Test Set\")\n# label_distribution(validation_ds, \"Sampled Validation Set\")\n# label_distribution(dataset[\"validation\"], \"Original Validation Set\")","metadata":{"execution":{"iopub.status.busy":"2025-03-20T17:05:09.832504Z","iopub.execute_input":"2025-03-20T17:05:09.832869Z","iopub.status.idle":"2025-03-20T17:05:10.177726Z","shell.execute_reply.started":"2025-03-20T17:05:09.832841Z","shell.execute_reply":"2025-03-20T17:05:10.176811Z"},"id":"hxMY1ac88MdD","trusted":true},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 1500\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 100\n    })\n    validation: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50\n    })\n})\nlabel\n0    437\n1    503\n2    122\n3    202\n4    182\n5     54\nName: count, dtype: int64\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAjsAAAHCCAYAAAAJowgXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA6OUlEQVR4nO3deVhWdf7/8Rc7AoKiAlKIihnilqEJtliKoqFG6piOCzZOiyEt/saMxnFroazRMheqby5ZjmmLpbnjMqVoimlqapoLjgZqBreKgsL5/dHFPd0CLnjDDWeej+u6r4vzOZ9zn/f5cIAX5/6c+3YyDMMQAACASTk7ugAAAICKRNgBAACmRtgBAACmRtgBAACmRtgBAACmRtgBAACmRtgBAACmRtgBAACmRtgBAACmRtgByuDk5KTx48fb7fnWr18vJycnrV+/3m7PWV5Dhw5Vw4YNy7Xt+PHj5eTkZN+C/kc1bNhQPXr0sOtz2vu8LVaZ529p55iTk5NGjBhR4fuWpDlz5sjJyUlHjhyplP2h4hF2UKF27dqlvn37KjQ0VJ6enrrlllvUpUsXvfPOO44urUpycnK6rkdVCEyOsmTJEnXs2FEBAQHy8vJS48aN1a9fP61YsaJcz/fqq69q8eLF19X3yJEjcnJy0ptvvlmufVUVxcdR/HBzc1PdunXVoUMHvfjii8rMzLTbvm5kfCtbVa4N9uXq6AJgXps2bdIDDzygBg0a6LHHHlNQUJCOHTumzZs36+2331ZSUpKjS6xy5s2bZ7P84YcfavXq1SXamzVrdlP7ef/991VUVFSubceMGaMXXnjhpvZfXm+++aZGjRqljh07Kjk5WV5eXjp48KDWrFmjBQsWqFu3bjf8nK+++qr69u2r+Ph4+xdcxQ0YMEAPPvigioqK9Ntvv2nr1q1666239Pbbb+uDDz5Q//79rX3vu+8+XbhwQe7u7je0j/KMb2WdY2XVNnjwYPXv318eHh4VXgMqB2EHFeaVV16Rn5+ftm7dqlq1atmsO3nypGOKquIGDRpks7x582atXr26RPuV8vLy5OXldd37cXNzK1d9kuTq6ipX18r/1XH58mW99NJL6tKli1atWlViPefUjbvzzjtLnFtHjx5V165dlZCQoGbNmql169aSJGdnZ3l6elZoPefPn5e3t7fDzrFiLi4ucnFxcdj+YX+8jIUK8/PPP6t58+Ylgo4kBQQE2CzPnj1bnTp1UkBAgDw8PBQREaGZM2eW2K54jsP69evVtm1b1ahRQy1btrS+rPP555+rZcuW8vT0VGRkpL7//nub7YcOHSofHx8dOnRIsbGx8vb2VnBwsCZOnCjDMK55TMePH9df/vIXBQYGysPDQ82bN9esWbNK9PvPf/6j+Ph4eXt7KyAgQM8995zy8/Ov+fzX4/7771eLFi2UkZGh++67T15eXnrxxRclSV9++aXi4uIUHBwsDw8PhYWF6aWXXlJhYWGJcfjjnJ0/vjzz3nvvKSwsTB4eHmrXrp22bt1qs+3V5lMsXrxYLVq0sI5NaS8tFX/vPD09FRYWpnffffe65gGdPn1aFotFd999d6nrrzyn8vPzNW7cODVp0kQeHh4KCQnR888/b/N9cHJy0vnz5zV37lzrSzpDhw69ah3X43rP52KrVq3SHXfcIU9PT0VEROjzzz8v0ScnJ0fPPvusQkJC5OHhoSZNmuj1118v9xW6soSGhmrOnDkqKCjQpEmTrO2lzdk5cOCA+vTpo6CgIHl6eurWW29V//79lZubK+nq41v8Pf/xxx/15z//WbVr19Y999xjs640H3/8sW6//Xbrz/i///1vm/VlzUe78jmvVltZc3ZmzJih5s2by8PDQ8HBwUpMTFROTo5Nn+Kfzx9//FEPPPCAvLy8dMstt9iMJSofV3ZQYUJDQ5Wenq7du3erRYsWV+07c+ZMNW/eXL169ZKrq6uWLFmip556SkVFRUpMTLTpe/DgQf35z3/WE088oUGDBunNN99Uz549lZqaqhdffFFPPfWUJCklJUX9+vXT/v375ez831xfWFiobt26KSoqSpMmTdKKFSs0btw4Xb58WRMnTiyzxuzsbEVFRVn/sNerV0/Lly/XsGHDZLFY9Oyzz0qSLly4oM6dOyszM1NPP/20goODNW/ePK1du7acI1nSr7/+qu7du6t///4aNGiQAgMDJf3+S9rHx0cjR46Uj4+P1q5dq7Fjx8piseiNN9645vPOnz9fZ8+e1RNPPCEnJydNmjRJvXv31qFDh655Nejbb7/V559/rqeeeko1a9bU1KlT1adPH2VmZqpOnTqSpO+//17dunVT/fr1NWHCBBUWFmrixImqV6/eNWsLCAhQjRo1tGTJEiUlJcnf37/MvkVFRerVq5e+/fZbPf7442rWrJl27dqlKVOm6KeffrLO05g3b57++te/6q677tLjjz8uSQoLC7tmLddyI+fzgQMH9Mgjj+jJJ59UQkKCZs+erT/96U9asWKFunTpIun3K3cdO3bU8ePH9cQTT6hBgwbatGmTkpOT9csvv+itt9666Zr/KDo6WmFhYVq9enWZfQoKChQbG6v8/HwlJSUpKChIx48f19KlS5WTkyM/P7/rGt8//elPuu222/Tqq69e8x+ODRs26JNPPtHTTz8tDw8PzZgxQ926ddN33313zd8xV7rR7/348eM1YcIExcTEaPjw4dq/f79mzpyprVu3auPGjTY/H7/99pu6deum3r17q1+/fvr00081evRotWzZUt27d7+hOmEnBlBBVq1aZbi4uBguLi5GdHS08fzzzxsrV640CgoKSvTNy8sr0RYbG2s0btzYpi00NNSQZGzatMnatnLlSkOSUaNGDePo0aPW9nfffdeQZKxbt87alpCQYEgykpKSrG1FRUVGXFyc4e7ubpw6dcraLskYN26cdXnYsGFG/fr1jdOnT9vU1L9/f8PPz896DG+99ZYhyVi4cKG1z/nz540mTZqUqOdaEhMTjSt/TDt27GhIMlJTU0v0L20cn3jiCcPLy8u4ePGitS0hIcEIDQ21Lh8+fNiQZNSpU8c4c+aMtf3LL780JBlLliyxto0bN65ETZIMd3d34+DBg9a2nTt3GpKMd955x9rWs2dPw8vLyzh+/Li17cCBA4arq2uJ5yzN2LFjDUmGt7e30b17d+OVV14xMjIySvSbN2+e4ezsbHzzzTc27ampqYYkY+PGjdY2b29vIyEh4Zr7Noz/jtMbb7xx1X43ej5/9tln1rbc3Fyjfv36Rps2baxtL730kuHt7W389NNPNtu/8MILhouLi5GZmWltu/K8Le9xPPTQQ4YkIzc31zAMw1i3bp3N+fv9998bkoxFixZddV9ljW/xeTRgwIAy1/2RJEOSsW3bNmvb0aNHDU9PT+Phhx+2tl15bl/tOcuqbfbs2YYk4/Dhw4ZhGMbJkycNd3d3o2vXrkZhYaG137Rp0wxJxqxZs6xtxT+fH374obUtPz/fCAoKMvr06VNiX6gcvIyFCtOlSxelp6erV69e2rlzpyZNmqTY2Fjdcsst+uqrr2z61qhRw/p1bm6uTp8+rY4dO+rQoUPWS+LFIiIiFB0dbV1u3769JKlTp05q0KBBifZDhw6VqO2Pt7AWX6kpKCjQmjVrSj0WwzD02WefqWfPnjIMQ6dPn7Y+YmNjlZubq+3bt0uSli1bpvr166tv377W7b28vKz/PdqDh4eHHn300RLtfxzHs2fP6vTp07r33nuVl5enffv2XfN5H3nkEdWuXdu6fO+990oqfQyvFBMTY/OfcatWreTr62vdtrCwUGvWrFF8fLyCg4Ot/Zo0aXLd/+1OmDBB8+fPV5s2bbRy5Ur9/e9/V2RkpO68807t3bvX2m/RokVq1qyZwsPDbb5XnTp1kiStW7fuuvZXXjdyPgcHB+vhhx+2Lvv6+mrIkCH6/vvvlZWVZT2ee++9V7Vr17Y5npiYGBUWFpZ4KccefHx8JP1+HpXGz89PkrRy5Url5eWVez9PPvnkdfeNjo5WZGSkdblBgwZ66KGHtHLlyhIv1drTmjVrVFBQoGeffdbmKvFjjz0mX19fff311zb9fXx8bOZCubu766677rqunyNUDF7GQoVq166dPv/8cxUUFGjnzp364osvNGXKFPXt21c7duxQRESEJGnjxo0aN26c0tPTS/zizM3Ntf5ilWQTaKT//tINCQkptf23336zaXd2dlbjxo1t2po2bSpJZb6vxqlTp5STk6P33ntP7733Xql9iifIHj16VE2aNCkx5+D2228vdbvyuOWWW0q9K2bPnj0aM2aM1q5dK4vFYrPuyj+ypblybIuDz5VjeD3bFm9fvO3Jkyd14cIFNWnSpES/0trKMmDAAA0YMEAWi0VbtmzRnDlzNH/+fPXs2VO7d++Wp6enDhw4oL1795b58lhFT2a+kfO5tHPlj+djUFCQDhw4oB9++KFSj+fcuXOSpJo1a5a6vlGjRho5cqQmT56sjz/+WPfee6969eqlQYMG2RzftTRq1Oi6+952220l2po2baq8vDydOnVKQUFB1/1cN+Lo0aOSSv4Mu7u7q3Hjxtb1xW699dYS39PatWvrhx9+qJD6cG2EHVQKd3d3tWvXTu3atVPTpk316KOPatGiRRo3bpx+/vlnde7cWeHh4Zo8ebJCQkLk7u6uZcuWacqUKSUmYJZ1l0RZ7cZ1TDy+luIaBg0apISEhFL7tGrV6qb3c73+eOWgWE5Ojjp27ChfX19NnDhRYWFh8vT01Pbt2zV69Ojrmsh6M2NYkeNfGl9fX3Xp0kVdunSRm5ub5s6dqy1btqhjx44qKipSy5YtNXny5FK3vTIY29ONns/Xo6ioSF26dNHzzz9f6vricGRPu3fvVkBAgHx9fcvs889//lNDhw7Vl19+qVWrVunpp59WSkqKNm/erFtvvfW69lPauXwzyprYXJFXfq5U2T8LuDbCDipd27ZtJUm//PKLpN/fJC4/P19fffWVzdWBinqpoaioSIcOHbL5A/HTTz9JUpnvKlyvXj3VrFlThYWFiomJuerzh4aGavfu3TIMw+YX7/79+2+++KtYv369fv31V33++ee67777rO2HDx+u0P1er4CAAHl6eurgwYMl1pXWdiPatm2ruXPnWs+psLAw7dy5U507d77mXV72fjfoGz2fDx48WOJcufJ8DAsL07lz56557tlLenq6fv7552u+5YEktWzZUi1bttSYMWO0adMm3X333UpNTdXLL78syb7je+DAgRJtP/30k7y8vKxXvWrXrl3iDilJJa6+3EhtoaGhkn7/Gf7jVeGCggIdPny40r4vKD/m7KDCrFu3rtT/ZJYtWybpv5eEi/8L+mPf3NxczZ49u8JqmzZtmvVrwzA0bdo0ubm5qXPnzqX2d3FxUZ8+ffTZZ59p9+7dJdafOnXK+vWDDz6oEydO6NNPP7W25eXllfnyl72UNo4FBQWaMWNGhe73erm4uCgmJkaLFy/WiRMnrO0HDx7U8uXLr7l9Xl6e0tPTS11XvH3xOdWvXz8dP35c77//fom+Fy5c0Pnz563L3t7epf5xLK8bPZ9PnDihL774wrpssVj04Ycf6o477rC+LNOvXz+lp6dr5cqVJbbPycnR5cuX7Vb/0aNHNXToULm7u2vUqFFl9rNYLCX227JlSzk7O9vc3m/P8U1PT7fOjZOkY8eO6csvv1TXrl2t4x4WFqbc3Fybl4x++eUXmzG+0dpiYmLk7u6uqVOn2nxfP/jgA+Xm5iouLu4mjgqVgSs7qDBJSUnKy8vTww8/rPDwcBUUFGjTpk365JNP1LBhQ+sE265du8rd3V09e/bUE088oXPnzun9999XQECA9T91e/L09NSKFSuUkJCg9u3ba/ny5fr666/14osvXvUW6Ndee03r1q1T+/bt9dhjjykiIkJnzpzR9u3btWbNGp05c0bS75MWp02bpiFDhigjI0P169fXvHnzbuhN/8qjQ4cOql27thISEvT000/LyclJ8+bNq1KXzsePH69Vq1bp7rvv1vDhw1VYWKhp06apRYsW2rFjx1W3zcvLU4cOHRQVFaVu3bopJCREOTk5Wrx4sb755hvFx8erTZs2kn5/B9yFCxfqySef1Lp163T33XersLBQ+/bt08KFC7Vy5UrrFcbIyEitWbNGkydPVnBwsBo1amSd3F6WtLQ0Xbx4sUR7fHz8DZ/PTZs21bBhw7R161YFBgZq1qxZys7OtglHo0aN0ldffaUePXpo6NChioyM1Pnz57Vr1y59+umnOnLkiOrWrXut4S9h+/bt+uijj1RUVKScnBxt3bpVn332mfXcudpLs2vXrtWIESP0pz/9SU2bNtXly5c1b9486z8GxcozvmVp0aKFYmNjbW49l36fuF6sf//+Gj16tB5++GE9/fTTysvL08yZM9W0aVOboHQjtdWrV0/JycmaMGGCunXrpl69emn//v2aMWOG2rVrd11XwOBgDrgDDP8jli9fbvzlL38xwsPDDR8fH8Pd3d1o0qSJkZSUZGRnZ9v0/eqrr4xWrVoZnp6eRsOGDY3XX3/dmDVrls3tn4bx+626cXFxJfYlyUhMTLRpK+322oSEBMPb29v4+eefja5duxpeXl5GYGCgMW7cOJtbSouf88pbeLOzs43ExEQjJCTEcHNzM4KCgozOnTsb7733nk2/o0ePGr169TK8vLyMunXrGs8884yxYsUKu9163rx581L7b9y40YiKijJq1KhhBAcHW2/3v3K/Zd16XtqtyFeOQ1m3BV85/obx+/frylt709LSjDZt2hju7u5GWFiY8X//93/G//t//8/w9PQsYxR+d+nSJeP999834uPjjdDQUMPDw8Pw8vIy2rRpY7zxxhtGfn6+Tf+CggLj9ddfN5o3b254eHgYtWvXNiIjI40JEyZYb6c2DMPYt2+fcd999xk1atQwJF31NvTicSrrMW/ePMMwbvx8XrlypdGqVSvDw8PDCA8PL/V27rNnzxrJyclGkyZNDHd3d6Nu3bpGhw4djDfffNPm7RxKO2+vdRyurq6Gv7+/0b59eyM5OdnmLRyKXXnr+aFDh4y//OUvRlhYmOHp6Wn4+/sbDzzwgLFmzRqb7coa3+Lz6I9v91DsaufYRx99ZNx2222Gh4eH0aZNm1J/nlatWmW0aNHCcHd3N26//Xbjo48+KvU5y6rtylvPi02bNs0IDw833NzcjMDAQGP48OHGb7/9ZtOnrJ/Psm6JR+VwMowq9G8fUMGGDh2qTz/91HqnCaqG+Ph47dmzp9Q5GQBws5izA6BSXbhwwWb5wIEDWrZsme6//37HFATA9JizA6BSNW7cWEOHDrW+P8nMmTPl7u5e5m3VAHCzCDsAKlW3bt30r3/9S1lZWfLw8FB0dLReffXVUt8wDgDsgTk7AADA1JizAwAATI2wAwAATI05O/r94wNOnDihmjVr2v2t4wEAQMUwDENnz55VcHCwzSfSX4mwo9/frr0iPxgQAABUnGPHjl31w2cJO5Jq1qwp6ffButon/AIAgKrDYrEoJCTE+ne8LIQd/feTb319fQk7AABUM9eagsIEZQAAYGqEHQAAYGqEHQAAYGqEHQAAYGqEHQAAYGqEHQAAYGqEHQAAYGqEHQAAYGqEHQAAYGqEHQAAYGqEHQAAYGqEHQAAYGqEHQAAYGoODTvjx4+Xk5OTzSM8PNy6/uLFi0pMTFSdOnXk4+OjPn36KDs72+Y5MjMzFRcXJy8vLwUEBGjUqFG6fPlyZR8KAACoolwdXUDz5s21Zs0a67Kr639Leu655/T1119r0aJF8vPz04gRI9S7d29t3LhRklRYWKi4uDgFBQVp06ZN+uWXXzRkyBC5ubnp1VdfrfRjAQAAVY/Dw46rq6uCgoJKtOfm5uqDDz7Q/Pnz1alTJ0nS7Nmz1axZM23evFlRUVFatWqVfvzxR61Zs0aBgYG644479NJLL2n06NEaP3683N3dK/twUAEavvC1o0u4piOvxTm6BABAGRw+Z+fAgQMKDg5W48aNNXDgQGVmZkqSMjIydOnSJcXExFj7hoeHq0GDBkpPT5ckpaenq2XLlgoMDLT2iY2NlcVi0Z49e8rcZ35+viwWi80DAACYk0PDTvv27TVnzhytWLFCM2fO1OHDh3Xvvffq7NmzysrKkru7u2rVqmWzTWBgoLKysiRJWVlZNkGneH3xurKkpKTIz8/P+ggJCbHvgQEAgCrDoS9jde/e3fp1q1at1L59e4WGhmrhwoWqUaNGhe03OTlZI0eOtC5bLBYCDwAAJuXwl7H+qFatWmratKkOHjyooKAgFRQUKCcnx6ZPdna2dY5PUFBQibuzipdLmwdUzMPDQ76+vjYPAABgTlUq7Jw7d04///yz6tevr8jISLm5uSktLc26fv/+/crMzFR0dLQkKTo6Wrt27dLJkyetfVavXi1fX19FRERUev0AAKDqcejLWH/729/Us2dPhYaG6sSJExo3bpxcXFw0YMAA+fn5adiwYRo5cqT8/f3l6+urpKQkRUdHKyoqSpLUtWtXRUREaPDgwZo0aZKysrI0ZswYJSYmysPDw5GHBgAAqgiHhp3//Oc/GjBggH799VfVq1dP99xzjzZv3qx69epJkqZMmSJnZ2f16dNH+fn5io2N1YwZM6zbu7i4aOnSpRo+fLiio6Pl7e2thIQETZw40VGHBAAAqhgnwzAMRxfhaBaLRX5+fsrNzWX+ThXE++wAAEpzvX+/q9ScHQAAAHsj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFNzdXQBZtXwha8dXcJ1OfJanKNLAACgQnFlBwAAmBphBwAAmBphBwAAmBphBwAAmBphBwAAmBphBwAAmBphBwAAmBphBwAAmBphBwAAmBphBwAAmBphBwAAmBphBwAAmBphBwAAmBphBwAAmBphBwAAmBphBwAAmBphBwAAmBphBwAAmBphBwAAmBphBwAAmBphBwAAmFqVCTuvvfaanJyc9Oyzz1rbLl68qMTERNWpU0c+Pj7q06ePsrOzbbbLzMxUXFycvLy8FBAQoFGjRuny5cuVXD0AAKiqqkTY2bp1q9599121atXKpv25557TkiVLtGjRIm3YsEEnTpxQ7969resLCwsVFxengoICbdq0SXPnztWcOXM0duzYyj4EAABQRTk87Jw7d04DBw7U+++/r9q1a1vbc3Nz9cEHH2jy5Mnq1KmTIiMjNXv2bG3atEmbN2+WJK1atUo//vijPvroI91xxx3q3r27XnrpJU2fPl0FBQWOOiQAAFCFODzsJCYmKi4uTjExMTbtGRkZunTpkk17eHi4GjRooPT0dElSenq6WrZsqcDAQGuf2NhYWSwW7dmzp8x95ufny2Kx2DwAAIA5uTpy5wsWLND27du1devWEuuysrLk7u6uWrVq2bQHBgYqKyvL2uePQad4ffG6sqSkpGjChAk3WT0AAKgOHHZl59ixY3rmmWf08ccfy9PTs1L3nZycrNzcXOvj2LFjlbp/AABQeRwWdjIyMnTy5EndeeedcnV1laurqzZs2KCpU6fK1dVVgYGBKigoUE5Ojs122dnZCgoKkiQFBQWVuDureLm4T2k8PDzk6+tr8wAAAObksLDTuXNn7dq1Szt27LA+2rZtq4EDB1q/dnNzU1pamnWb/fv3KzMzU9HR0ZKk6Oho7dq1SydPnrT2Wb16tXx9fRUREVHpxwQAAKoeh83ZqVmzplq0aGHT5u3trTp16ljbhw0bppEjR8rf31++vr5KSkpSdHS0oqKiJEldu3ZVRESEBg8erEmTJikrK0tjxoxRYmKiPDw8Kv2YAABA1ePQCcrXMmXKFDk7O6tPnz7Kz89XbGysZsyYYV3v4uKipUuXavjw4YqOjpa3t7cSEhI0ceJEB1YNAACqkioVdtavX2+z7OnpqenTp2v69OllbhMaGqply5ZVcGUAAKC6cvj77AAAAFQkwg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1h4admTNnqlWrVvL19ZWvr6+io6O1fPly6/qLFy8qMTFRderUkY+Pj/r06aPs7Gyb58jMzFRcXJy8vLwUEBCgUaNG6fLly5V9KAAAoIpyaNi59dZb9dprrykjI0Pbtm1Tp06d9NBDD2nPnj2SpOeee05LlizRokWLtGHDBp04cUK9e/e2bl9YWKi4uDgVFBRo06ZNmjt3rubMmaOxY8c66pAAAEAV42QYhuHoIv7I399fb7zxhvr27at69epp/vz56tu3ryRp3759atasmdLT0xUVFaXly5erR48eOnHihAIDAyVJqampGj16tE6dOiV3d/fr2qfFYpGfn59yc3Pl6+trl+No+MLXdnmeinbktThHl3BN1WEsq8M4AoDZXO/f7yozZ6ewsFALFizQ+fPnFR0drYyMDF26dEkxMTHWPuHh4WrQoIHS09MlSenp6WrZsqU16EhSbGysLBaL9eoQAAD43+bq6AJ27dql6OhoXbx4UT4+Pvriiy8UERGhHTt2yN3dXbVq1bLpHxgYqKysLElSVlaWTdApXl+8riz5+fnKz8+3LlssFjsdDQAAqGocfmXn9ttv144dO7RlyxYNHz5cCQkJ+vHHHyt0nykpKfLz87M+QkJCKnR/AADAcRwedtzd3dWkSRNFRkYqJSVFrVu31ttvv62goCAVFBQoJyfHpn92draCgoIkSUFBQSXuzipeLu5TmuTkZOXm5lofx44ds+9BAQCAKsPhYedKRUVFys/PV2RkpNzc3JSWlmZdt3//fmVmZio6OlqSFB0drV27dunkyZPWPqtXr5avr68iIiLK3IeHh4f1dvfiBwAAMCeHztlJTk5W9+7d1aBBA509e1bz58/X+vXrtXLlSvn5+WnYsGEaOXKk/P395evrq6SkJEVHRysqKkqS1LVrV0VERGjw4MGaNGmSsrKyNGbMGCUmJsrDw8ORhwYAAKoIh4adkydPasiQIfrll1/k5+enVq1aaeXKlerSpYskacqUKXJ2dlafPn2Un5+v2NhYzZgxw7q9i4uLli5dquHDhys6Olre3t5KSEjQxIkTHXVIAACgiqly77PjCLzPTtVWHcayOowjAJhNtXufHQAAgIpQrrDTuHFj/frrryXac3Jy1Lhx45suCgAAwF7KFXaOHDmiwsLCEu35+fk6fvz4TRcFAABgLzc0Qfmrr76yfl18x1SxwsJCpaWlqWHDhnYrDgAA4GbdUNiJj4+XJDk5OSkhIcFmnZubmxo2bKh//vOfdisOAADgZt1Q2CkqKpIkNWrUSFu3blXdunUrpCgAAAB7Kdf77Bw+fNjedQAAAFSIcr+pYFpamtLS0nTy5EnrFZ9is2bNuunCAAAA7KFcYWfChAmaOHGi2rZtq/r168vJycnedQEAANhFucJOamqq5syZo8GDB9u7HgAAALsq1/vsFBQUqEOHDvauBQAAwO7KFXb++te/av78+fauBQAAwO7K9TLWxYsX9d5772nNmjVq1aqV3NzcbNZPnjzZLsUBAADcrHKFnR9++EF33HGHJGn37t0265isDAAAqpJyhZ1169bZuw4AAIAKUa45OwAAANVFua7sPPDAA1d9uWrt2rXlLggAAMCeyhV2iufrFLt06ZJ27Nih3bt3l/iAUAAAAEcqV9iZMmVKqe3jx4/XuXPnbqogAAAAe7LrnJ1BgwbxuVgAAKBKsWvYSU9Pl6enpz2fEgAA4KaU62Ws3r172ywbhqFffvlF27Zt0z/+8Q+7FAYAAGAP5Qo7fn5+NsvOzs66/fbbNXHiRHXt2tUuhQEAANhDucLO7Nmz7V0HAABAhShX2CmWkZGhvXv3SpKaN2+uNm3a2KUoAAAAeylX2Dl58qT69++v9evXq1atWpKknJwcPfDAA1qwYIHq1atnzxoBAADKrVx3YyUlJens2bPas2ePzpw5ozNnzmj37t2yWCx6+umn7V0jAABAuZXrys6KFSu0Zs0aNWvWzNoWERGh6dOnM0EZAABUKeW6slNUVCQ3N7cS7W5ubioqKrrpogAAAOylXGGnU6dOeuaZZ3TixAlr2/Hjx/Xcc8+pc+fOdisOAADgZpUr7EybNk0Wi0UNGzZUWFiYwsLC1KhRI1ksFr3zzjv2rhEAAKDcyjVnJyQkRNu3b9eaNWu0b98+SVKzZs0UExNj1+IAAABu1g1d2Vm7dq0iIiJksVjk5OSkLl26KCkpSUlJSWrXrp2aN2+ub775pqJqBQAAuGE3FHbeeustPfbYY/L19S2xzs/PT0888YQmT55st+IAAABu1g2FnZ07d6pbt25lru/atasyMjJuuigAAAB7uaGwk52dXeot58VcXV116tSpmy4KAADAXm4o7Nxyyy3avXt3met/+OEH1a9f/6aLAgAAsJcbCjsPPvig/vGPf+jixYsl1l24cEHjxo1Tjx497FYcAADAzbqhW8/HjBmjzz//XE2bNtWIESN0++23S5L27dun6dOnq7CwUH//+98rpFAAAIDyuKGwExgYqE2bNmn48OFKTk6WYRiSJCcnJ8XGxmr69OkKDAyskEIBAADK44bfVDA0NFTLli3Tb7/9poMHD8owDN12222qXbt2RdQHAABwU8r1DsqSVLt2bbVr186etQAAANhduT4bCwAAoLog7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFNzaNhJSUlRu3btVLNmTQUEBCg+Pl779++36XPx4kUlJiaqTp068vHxUZ8+fZSdnW3TJzMzU3FxcfLy8lJAQIBGjRqly5cvV+ahAACAKsqhYWfDhg1KTEzU5s2btXr1al26dEldu3bV+fPnrX2ee+45LVmyRIsWLdKGDRt04sQJ9e7d27q+sLBQcXFxKigo0KZNmzR37lzNmTNHY8eOdcQhAQCAKsbJMAzD0UUUO3XqlAICArRhwwbdd999ys3NVb169TR//nz17dtXkrRv3z41a9ZM6enpioqK0vLly9WjRw+dOHFCgYGBkqTU1FSNHj1ap06dkru7+zX3a7FY5Ofnp9zcXPn6+trlWBq+8LVdnqeiHXktztElXFN1GMvqMI4AYDbX+/fbtRJruqbc3FxJkr+/vyQpIyNDly5dUkxMjLVPeHi4GjRoYA076enpatmypTXoSFJsbKyGDx+uPXv2qE2bNpV7EABMrzoEcIkQDhSrMmGnqKhIzz77rO6++261aNFCkpSVlSV3d3fVqlXLpm9gYKCysrKsff4YdIrXF68rTX5+vvLz863LFovFXocBAACqmCpzN1ZiYqJ2796tBQsWVPi+UlJS5OfnZ32EhIRU+D4BAIBjVImwM2LECC1dulTr1q3Trbfeam0PCgpSQUGBcnJybPpnZ2crKCjI2ufKu7OKl4v7XCk5OVm5ubnWx7Fjx+x4NAAAoCpxaNgxDEMjRozQF198obVr16pRo0Y26yMjI+Xm5qa0tDRr2/79+5WZmano6GhJUnR0tHbt2qWTJ09a+6xevVq+vr6KiIgodb8eHh7y9fW1eQAAAHNy6JydxMREzZ8/X19++aVq1qxpnWPj5+enGjVqyM/PT8OGDdPIkSPl7+8vX19fJSUlKTo6WlFRUZKkrl27KiIiQoMHD9akSZOUlZWlMWPGKDExUR4eHo48PAAAUAU4NOzMnDlTknT//ffbtM+ePVtDhw6VJE2ZMkXOzs7q06eP8vPzFRsbqxkzZlj7uri4aOnSpRo+fLiio6Pl7e2thIQETZw4sbIOAwAAVGEODTvX8xY/np6emj59uqZPn15mn9DQUC1btsyepQEAAJOoEhOUAQAAKgphBwAAmBphBwAAmBphBwAAmBphBwAAmBphBwAAmFqV+SBQAMD/nurwCfJ8enz1x5UdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaq6OLgBA5Wn4wteOLuGajrwW5+gSAJgMV3YAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpOTTs/Pvf/1bPnj0VHBwsJycnLV682Ga9YRgaO3as6tevrxo1aigmJkYHDhyw6XPmzBkNHDhQvr6+qlWrloYNG6Zz585V4lEAAICqzKFh5/z582rdurWmT59e6vpJkyZp6tSpSk1N1ZYtW+Tt7a3Y2FhdvHjR2mfgwIHas2ePVq9eraVLl+rf//63Hn/88co6BAAAUMU59INAu3fvru7du5e6zjAMvfXWWxozZoweeughSdKHH36owMBALV68WP3799fevXu1YsUKbd26VW3btpUkvfPOO3rwwQf15ptvKjg4uNKOBQAAVE1Vds7O4cOHlZWVpZiYGGubn5+f2rdvr/T0dElSenq6atWqZQ06khQTEyNnZ2dt2bKl0msGAABVj0Ov7FxNVlaWJCkwMNCmPTAw0LouKytLAQEBNutdXV3l7+9v7VOa/Px85efnW5ctFou9ygYAAFVMlb2yU5FSUlLk5+dnfYSEhDi6JAAAUEGqbNgJCgqSJGVnZ9u0Z2dnW9cFBQXp5MmTNusvX76sM2fOWPuUJjk5Wbm5udbHsWPH7Fw9AACoKqps2GnUqJGCgoKUlpZmbbNYLNqyZYuio6MlSdHR0crJyVFGRoa1z9q1a1VUVKT27duX+dweHh7y9fW1eQAAAHNy6Jydc+fO6eDBg9blw4cPa8eOHfL391eDBg307LPP6uWXX9Ztt92mRo0a6R//+IeCg4MVHx8vSWrWrJm6deumxx57TKmpqbp06ZJGjBih/v37cycWAACQ5OCws23bNj3wwAPW5ZEjR0qSEhISNGfOHD3//PM6f/68Hn/8ceXk5Oiee+7RihUr5Onpad3m448/1ogRI9S5c2c5OzurT58+mjp1aqUfCwAAqJocGnbuv/9+GYZR5nonJydNnDhREydOLLOPv7+/5s+fXxHlAQAAE6iyc3YAAADsgbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMzdXRBQAAgJvX8IWvHV3CNR15Lc4h++XKDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXCDgAAMDXThJ3p06erYcOG8vT0VPv27fXdd985uiQAAFAFmCLsfPLJJxo5cqTGjRun7du3q3Xr1oqNjdXJkycdXRoAAHAwU4SdyZMn67HHHtOjjz6qiIgIpaamysvLS7NmzXJ0aQAAwMGqfdgpKChQRkaGYmJirG3Ozs6KiYlRenq6AysDAABVgaujC7hZp0+fVmFhoQIDA23aAwMDtW/fvlK3yc/PV35+vnU5NzdXkmSxWOxWV1F+nt2eqyLZ85grSnUYy+owjhJjaS/VYRwlxtJeqsM4Sv+bY1n8fIZhXLVftQ875ZGSkqIJEyaUaA8JCXFANY7l95ajKzAHxtF+GEv7YSztg3G0n4oay7Nnz8rPz6/M9dU+7NStW1cuLi7Kzs62ac/OzlZQUFCp2yQnJ2vkyJHW5aKiIp05c0Z16tSRk5NThdZbXhaLRSEhITp27Jh8fX0dXU61xljaB+NoP4yl/TCW9lFdxtEwDJ09e1bBwcFX7Vftw467u7siIyOVlpam+Ph4Sb+Hl7S0NI0YMaLUbTw8POTh4WHTVqtWrQqu1D58fX2r9IlXnTCW9sE42g9jaT+MpX1Uh3G82hWdYtU+7EjSyJEjlZCQoLZt2+quu+7SW2+9pfPnz+vRRx91dGkAAMDBTBF2HnnkEZ06dUpjx45VVlaW7rjjDq1YsaLEpGUAAPC/xxRhR5JGjBhR5stWZuDh4aFx48aVePkNN46xtA/G0X4YS/thLO3DbOPoZFzrfi0AAIBqrNq/qSAAAMDVEHYAAICpEXYAAICpEXYAAKbBNFSUxjR3Y5nN6dOnNWvWLKWnpysrK0uSFBQUpA4dOmjo0KGqV6+egysEgKrHw8NDO3fuVLNmzRxdCqoQ7saqgrZu3arY2Fh5eXkpJibG+n5B2dnZSktLU15enlauXKm2bds6uFL8L7lw4YIyMjLk7++viIgIm3UXL17UwoULNWTIEAdVV73s3btXmzdvVnR0tMLDw7Vv3z69/fbbys/P16BBg9SpUydHl1jl/fEjf/7o7bff1qBBg1SnTh1J0uTJkyuzLFM4f/68Fi5cqIMHD6p+/foaMGCAdTyrK8JOFRQVFaXWrVsrNTW1xGd1GYahJ598Uj/88IPS09MdVKF5HDt2TOPGjdOsWbMcXUqV9tNPP6lr167KzMyUk5OT7rnnHi1YsED169eX9HsQDw4OVmFhoYMrrfpWrFihhx56SD4+PsrLy9MXX3yhIUOGqHXr1ioqKtKGDRu0atUqAs81ODs7q3Xr1iU+6mfDhg1q27atvL295eTkpLVr1zqmwGokIiJC3377rfz9/XXs2DHdd999+u2339S0aVP9/PPPcnV11ebNm9WoUSNHl1p+BqocT09PY+/evWWu37t3r+Hp6VmJFZnXjh07DGdnZ0eXUeXFx8cbcXFxxqlTp4wDBw4YcXFxRqNGjYyjR48ahmEYWVlZjON1io6ONv7+978bhmEY//rXv4zatWsbL774onX9Cy+8YHTp0sVR5VUbKSkpRqNGjYy0tDSbdldXV2PPnj0Oqqp6cnJyMrKzsw3DMIyBAwcaHTp0MHJycgzDMIyzZ88aMTExxoABAxxZ4k1jzk4VFBQUpO+++07h4eGlrv/uu+/4KIzr9NVXX111/aFDhyqpkupt06ZNWrNmjerWrau6detqyZIleuqpp3Tvvfdq3bp18vb2dnSJ1caePXv04YcfSpL69eunwYMHq2/fvtb1AwcO1OzZsx1VXrXxwgsvqHPnzho0aJB69uyplJQUubm5Obqsai89PV2pqanWD9f08fHRhAkT1L9/fwdXdnMIO1XQ3/72Nz3++OPKyMhQ586dS8zZef/99/Xmm286uMrqIT4+Xk5OTle9Q+PKlwpR0oULF+Tq+t9fF05OTpo5c6ZGjBihjh07av78+Q6srvopPuecnZ3l6elp86nNNWvWVG5urqNKq1batWunjIwMJSYmqm3btvr444/5eS6n4nG7ePGi9eXpYrfccotOnTrliLLshrBTBSUmJqpu3bqaMmWKZsyYYZ0H4eLiosjISM2ZM0f9+vVzcJXVQ/369TVjxgw99NBDpa7fsWOHIiMjK7mq6ic8PFzbtm0rcYfLtGnTJEm9evVyRFnVUsOGDXXgwAGFhYVJ+v0/6QYNGljXZ2Zmlvhjg7L5+Pho7ty5WrBggWJiYpg3Vk6dO3eWq6urLBaL9u/frxYtWljXHT16tNpPUCbsVFGPPPKIHnnkEV26dEmnT5+WJNWtW5fLtDcoMjJSGRkZZYada131we8efvhh/etf/9LgwYNLrJs2bZqKioqUmprqgMqqn+HDh9v8Qf7jHxVJWr58OZOTy6F///665557lJGRodDQUEeXU62MGzfOZtnHx8dmecmSJbr33nsrsyS7424smNo333yj8+fPq1u3bqWuP3/+vLZt26aOHTtWcmUAgMpC2AEAAKbGx0UAAABTI+wAAABTI+wAAABTI+wAMKU5c+aU+CiB8nByctLixYtv+nkAOA5hB0CVNXToUMXHxzu6DADVHGEHAACYGmEHQLU0efJktWzZUt7e3goJCdFTTz2lc+fOlei3ePFi3XbbbfL09FRsbKyOHTtms/7LL7/UnXfeKU9PTzVu3FgTJkzQ5cuXK+swAFQCwg6AasnZ2VlTp07Vnj17NHfuXK1du1bPP/+8TZ+8vDy98sor+vDDD7Vx40bl5OTYfKDhN998oyFDhuiZZ57Rjz/+qHfffVdz5szRK6+8UtmHA6AC8aaCAKqsoUOHKicn57omCH/66ad68sknrR+vMmfOHD366KPavHmz2rdvL0nat2+fmjVrpi1btuiuu+5STEyMOnfurOTkZOvzfPTRR3r++ed14sQJSb9PUP7iiy+YOwRUY3w2FoBqac2aNUpJSdG+fftksVh0+fJlXbx4UXl5efLy8pIkubq6ql27dtZtwsPDVatWLe3du1d33XWXdu7cqY0bN9pcySksLCzxPACqN8IOgGrnyJEj6tGjh4YPH65XXnlF/v7++vbbbzVs2DAVFBRcd0g5d+6cJkyYoN69e5dY5+npae+yATgIYQdAtZORkaGioiL985//lLPz71MPFy5cWKLf5cuXtW3bNt11112SpP379ysnJ0fNmjWTJN15553av3+/mjRpUnnFA6h0hB0AVVpubq527Nhh01a3bl1dunRJ77zzjnr27KmNGzcqNTW1xLZubm5KSkrS1KlT5erqqhEjRigqKsoafsaOHasePXqoQYMG6tu3r5ydnbVz507t3r1bL7/8cmUcHoBKwN1YAKq09evXq02bNjaPefPmafLkyXr99dfVokULffzxx0pJSSmxrZeXl0aPHq0///nPuvvuu+Xj46NPPvnEuj42NlZLly7VqlWr1K5dO0VFRWnKlCkKDQ2tzEMEUMG4GwsAAJgaV3YAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICp/X8nJdr1qZoAcQAAAABJRU5ErkJggg==\n"},"metadata":{}},{"name":"stdout","text":"label\n0    4666\n1    5362\n2    1304\n3    2159\n4    1937\n5     572\nName: count, dtype: int64\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":27},{"cell_type":"markdown","source":"#### Q2.1: Preparing Data for Fine-Tuning (10 pts)","metadata":{"id":"YiZX6nlULxUe"}},{"cell_type":"markdown","source":"a. Let's get the emotion dataset ready for LoRA fine-tuning. Here's what you need to do:\n\n1.  **Format the Data**: Turn each data entry into a conversation like this:\n    *   A system instruction that tells the model what to do (analyze emotions)```*```.\n    *   A user query that gives the model the text to analyze.\n    *   An assistant response that provides the correct emotion label (in natural language, naturally!)\n2.  **Tokenize and Label**:\n    *   Tokenize the formatted conversation.\n    *   Prepare labels for training, make sure to mask the instruction part of the data ```**```.\n\nAlso, write a verification function that in a human readable format:\n\n*   Prints the complete training input sequence after tokenization for a given data entry.\n*   Shows the labels, indicating which tokens are being predicted.\n*   Checks if the assistant header is correctly handled by finding its position in the text and printing the subsequent text.\n\n```*TIP:``` It is a good practice to make your system instruction as concise as possible. For example in this task, you should tell the LLM explicitly that what are the valid labels.","metadata":{"id":"6emefGZV8MdD"}},{"cell_type":"markdown","source":"b. When preparing the data, experiment with the tokenizer parameters, namely `truncation`, `padding` and `max_length`. In a ```concise``` manner, explain what each one of them does and what is a good value and why.","metadata":{"id":"hDtRf-Bx8MdD"}},{"cell_type":"markdown","source":"```truncation```: This is a value that tells the tokenizer whether to shorten a sequence that exceeds the specified maximum length. Setting it to \"True\" or \"longest_first\" will do well and limit the sequence length appropriately.\n\n```padding```: This determines whther the tokenizer adds padding token such as [PAD] so that all sequences have the same length. Setting it to True makes sense to perform this action.\n\n```max_length```: This determines the maximumn token limit of the sequences. A good value for this parameter could be something like 512 depending on the model.\n\n\n\n","metadata":{"id":"-BBTOIWywisS"}},{"cell_type":"markdown","source":"c. ```**```When preparing the data, mask the instruction part of the data (set labels to -100 for the instruction tokens) before starting the training. Why is this a good idea?","metadata":{"id":"7hVKUSxK8MdD"}},{"cell_type":"markdown","source":"By setting the label of -100 for the instruction tokens, these tokens are masked for the loss calculation. This prevents the model from learning to output the instruction tokens. Why is this a good idea? Because our focus here is that the model learns to predict emotions and we do not need the model to output the instruction tokens. The instruction tokens are only here to help the model give a consistent prediction and should not appear in the output itself. By doing this, the model can then only focus on its assistant aspect of predicting the actual emotion label. This will also help in the generalization of the model by not learning the static instructions that do not change.","metadata":{"id":"7Mverm_SwloH"}},{"cell_type":"code","source":"### Your Code Here\n\n\ndef chatML_format(example, system_instruction, label_names_dict):\n    return [\n        {\"role\": \"system\", \"content\": system_instruction},\n        {\"role\": \"user\", \"content\": example[\"text\"]},\n        {\"role\": \"assistant\", \"content\": f\"The emotion is {label_names_dict[example['label']]}.\"}\n    ]\n\ndef find_assistant(tokenizer, input_ids):\n    pattern = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n    pattern_id = tokenizer.encode(pattern, add_special_tokens=False)\n    text_ids = input_ids[0].tolist()\n     \n    # Go through encoded tokens looknig for\n    # the end point of the assistant pattern\n    for i in range(len(text_ids) - len(pattern_id) + 1):\n        if text_ids[i : i + len(pattern_id)] == pattern_id:\n            return i + len(pattern_id)\n\n    print(text_ids)\n    print(pattern_id)\n    \n    return -1\n\ndef instruction_tokenize(batch, label_names_dict, system_instruction, tokenizer):\n    batch_size = len(batch[\"text\"])\n    \n    # Create chatML formatted data first\n    chatml_list = []\n    for i in range(batch_size):\n        \n        # Convert to chatML format\n        example = {\"text\" : batch[\"text\"][i], \"label\" : batch[\"label\"][i]}\n        chat_ml = chatML_format(example, system_instruction, label_names_dict)\n        chatml_list.append(chat_ml)\n    \n    formatted_texts = [tokenizer.apply_chat_template(convo, tokenize=False) for convo in chatml_list]\n    \n    # Convert to chat template and tokenize in batch\n    inputs = tokenizer(\n        formatted_texts,\n        padding=\"max_length\",   \n        truncation=True,\n        max_length=256, \n        return_tensors=\"pt\"\n    )\n\n    # Find the assistant part\n    pos_list = []\n    for i in range(len(chatml_list)):\n        pos = find_assistant(tokenizer, inputs[\"input_ids\"][i].unsqueeze(0))\n        if pos == -1:\n            raise ValueError(\"Assistant token not found in input!\")\n        pos_list.append(pos)\n\n    # Mask instruction before the assistant output\n    inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n    for i in range(len(pos_list)):\n        inputs[\"labels\"][i, 0 : pos_list[i]] = -100\n    \n    return inputs\n\n\ndef verify_tokenized_entry(example, tokenizer, index):\n    input_ids = example[\"input_ids\"][index]\n    labels = example[\"labels\"][index]\n\n    ### Print the complete training input sequence after tokenization for a given data entry.\n    input_seq = tokenizer.decode(input_ids)\n    print(f\"[*] Complete training input sequence after tokenization:\\n {input_seq} \\n\\n\")\n\n    ### Show the labels, indicating which tokens are being predicted.\n    print(\"[*] Label ids along with their decoded tokens and whether they are predicted or not (read in order of rows):\")\n    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    for i in range(0, len(tokens), 2):\n        log = \"\"\n        if labels[i] != -100:\n            log += f\"{labels[i]} : {tokens[i]} ==> Predicted\"\n        else:\n            log += f\"{labels[i]} ==> Not Predicted\"\n\n        if i + 1 < len(tokens):\n            if labels[i + 1] != -100:\n                log += f\", {labels[i + 1]} : {tokens[i + 1]} ==> Predicted\"\n            else:\n                log += f\", {labels[i + 1]} ==> Not Predicted\"\n        print(log)\n    print(\"\\n\")\n    \n    ### Checks if the assistant header is correctly handled by finding its position in the text and printing the subsequent text.\n    input_ids_tensor = torch.tensor([input_ids])  \n    pos = find_assistant(tokenizer, input_ids_tensor)\n    \n    if pos == -1:\n        raise ValueError(\"Assistant token not found in input!\")\n\n    assistant_response = tokenizer.decode(input_ids[pos:])\n    print(f\"[*] Assistant response after the assistant head:\\n{assistant_response}\")\n\n\n# Label names dictionary\nlabel_names_dict = dict(zip(range(6), dataset[\"train\"].features[\"label\"].names))\n\n# System instruction\nsystem_instruction = \"Analyze the emotion in the text. Choose from: sadness, joy, \" +\\\n\"love, anger, fear, surprise. Respond with \\\"The emotion is [label].\\\"\"\n\ntokenized_dataset = sample_dataset.map(\n    instruction_tokenize,\n    batched=True,\n    batch_size=2,\n    fn_kwargs={\n        \"label_names_dict\": label_names_dict,\n        \"system_instruction\": system_instruction,\n        \"tokenizer\": tokenizer,\n    },\n    remove_columns=sample_dataset[\"train\"].column_names\n)\n\ntokenized_dataset","metadata":{"id":"gkwNaJCY8MdD","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T17:07:16.266290Z","iopub.execute_input":"2025-03-20T17:07:16.266657Z","iopub.status.idle":"2025-03-20T17:07:19.004100Z","shell.execute_reply.started":"2025-03-20T17:07:16.266615Z","shell.execute_reply":"2025-03-20T17:07:19.003239Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57149ea55f8d42d7af46d3ba2bb5b985"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f723c7e86507412586079a30165596ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d64c1cba6eaa44ecb0f65d5197af0329"}},"metadata":{}},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 1500\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 100\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 50\n    })\n})"},"metadata":{}}],"execution_count":32},{"cell_type":"markdown","source":"d. Run your verification function on the first sample of your training dataset to see everything is in order.","metadata":{"id":"_6BnCCXC8MdE"}},{"cell_type":"code","source":"### Your Code Here\n\n# Run verification function on the first sample of training dataset\nverify_tokenized_entry(tokenized_dataset[\"train\"], tokenizer, 0)","metadata":{"execution":{"iopub.status.busy":"2025-03-20T17:07:26.014995Z","iopub.execute_input":"2025-03-20T17:07:26.015284Z","iopub.status.idle":"2025-03-20T17:07:26.357473Z","shell.execute_reply.started":"2025-03-20T17:07:26.015262Z","shell.execute_reply":"2025-03-20T17:07:26.356661Z"},"id":"WySUgurB8MdE","trusted":true},"outputs":[{"name":"stdout","text":"[*] Complete training input sequence after tokenization:\n <|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 20 Mar 2025\n\nAnalyze the emotion in the text. Choose from: sadness, joy, love, anger, fear, surprise. Respond with \"The emotion is [label].\"<|eot_id|><|start_header_id|>user<|end_header_id|>\n\ni will gladly endure a million emotional blowouts and tantrums for the privilege of feeling her tender hands in mine<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nThe emotion is love.<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|> \n\n\n[*] Label ids along with their decoded tokens and whether they are predicted or not (read in order of rows):\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n-100 ==> Not Predicted, -100 ==> Not Predicted\n791 : The ==> Predicted, 20356 : Ġemotion ==> Predicted\n374 : Ġis ==> Predicted, 3021 : Ġlove ==> Predicted\n13 : . ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n128009 : <|eot_id|> ==> Predicted, 128009 : <|eot_id|> ==> Predicted\n\n\n[*] Assistant response after the assistant head:\nThe emotion is love.<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"### B. Fine-tune using LoRa (30 pts)","metadata":{"id":"IbkYwTvAR70S"}},{"cell_type":"code","source":"\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n","metadata":{"execution":{"iopub.status.busy":"2025-03-20T17:08:56.290369Z","iopub.execute_input":"2025-03-20T17:08:56.290694Z","iopub.status.idle":"2025-03-20T17:08:56.295055Z","shell.execute_reply.started":"2025-03-20T17:08:56.290669Z","shell.execute_reply":"2025-03-20T17:08:56.294214Z"},"id":"v6aLrrmnR70S","trusted":true},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"#### Q2.2: Experimenting with LoRA Configuration Parameters (3 pts)","metadata":{"id":"OmYqjqIJ8MdE"}},{"cell_type":"markdown","source":"In this section, you may explore the effect of different LoRA configuration parameters on the trainable parameter count:\n\n1. Try different rank values (`r`) - experiment with values like 8, 16, 32, and 64\n    - Higher rank allows for more expressive power but increases parameter count\n    \n2. Adjust the scaling factor (`lora_alpha`) - typically set to 2x the rank\n    - This affects the magnitude of updates during training\n    \n3. Modify target modules - test different combinations like:\n    - Only attention modules: `[\"q_proj\", \"v_proj\"]`\n    - All attention modules: `[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]`\n    - Including feed-forward: `[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]`\n    \n4. Vary dropout rates (`lora_dropout`) - test values like 0.0, 0.05, 0.1\n    - Higher dropout can help with regularization\n\nYou may use the `print_trainable_parameters()` function to observe how each change affects the number of trainable parameters.\n\n(We are not requiring you to print and explain everything, these are some values to help you out)\n\na. Find a configuration that provides a good balance between parameter efficiency and model expressiveness. Explain your reasons in a concise manner.\n","metadata":{"id":"CxVKOKLP8MdE"}},{"cell_type":"markdown","source":"Given my experiments, I had a lot of difficulty with the VRAM available to me. So over time I tried to decrease whatever that could cause the model to take more VRAM. Therefore, I finally chose a small value of r=2 for the rank and of course choose alpha to be twice that. And in which modules to choose, I chose the middle choice. Not every module, and not as limited as the first choice. It mades sense to keep both efficiency and accuracy of the model. I am sure with more experimentation better hyperparameters could have been chosen to both speed up training and train a more accurate model.","metadata":{"id":"78-QYYMRx1kx"}},{"cell_type":"code","source":"### Your Answer Here (Final Chosen Lora Config + Output of trainable parameters function on that)\n\n# Load the model (Llama-3.2-1B)\nmodel, tokenizer = new_model(model, tokenizer, BASE_MODEL)\n\nprint(\"[*] Parameters of the base model:\")\nprint_trainable_parameters(model)\n\n# LORA config\nlora_config = LoraConfig(\n    r=2,\n    lora_alpha=4,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM\n)\n\n# Apply lora to architecture\nlora_model = get_peft_model(model, lora_config)\n\n# Print the parameters\nprint(\"\\n[*] Parameters of the lora model:\")\nprint_trainable_parameters(lora_model)","metadata":{"execution":{"iopub.status.busy":"2025-03-20T17:11:17.381332Z","iopub.execute_input":"2025-03-20T17:11:17.381717Z","iopub.status.idle":"2025-03-20T17:11:26.785887Z","shell.execute_reply.started":"2025-03-20T17:11:17.381685Z","shell.execute_reply":"2025-03-20T17:11:26.785142Z"},"id":"b42nIsQs8MdE","trusted":true},"outputs":[{"name":"stdout","text":"[*] Parameters of the base model:\ntrainable params: 1235814400 || all params: 1235814400 || trainable%: 100.0\n\n[*] Parameters of the lora model:\ntrainable params: 425984 || all params: 1236240384 || trainable%: 0.03445802333537099\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"#### Q2.3: Training Callbacks and Early Stopping (10 pts)","metadata":{"id":"9sqV1RFl8MdE"}},{"cell_type":"markdown","source":"\n**Understanding Training Callbacks**\n\nGenerally speaking, in deep learning, callbacks are functions that can be applied at various stages of training\n(start/end of training, epoch, or batch) to modify the training process. They're powerful\ntools that allow you to:\n\n- Monitor training metrics in real-time\n- Add custom logging\n- Save model checkpoints\n- Implement early stopping\n- Adjust learning rates dynamically\n\n**Early Stopping**\n\nEarly stopping is a regularization technique that prevents overfitting by stopping training\nwhen a monitored metric stops improving. Benefits include:\n\n- Reduced training time\n- Better generalization\n- Prevention of overfitting\n\n**Your Task**\n\na. Implement a custom callback class that:\n1. Tracks the best loss value during training\n2. Calculates perplexity in steps\n3. Adds perplexity to the training logs\n4. Implements early stopping if the loss doesn't improve for several steps (This is called patience)\n5. (In your final project it is a good idea to use the big enough validation set to better monitor the training process. Given the time constraints for this assignment, we are not requiring you to do that.)\n\n***NOTE:*** You should inherit from the TrainerCallback class implemented in transformers\n","metadata":{"id":"xHORrpBU8MdE"}},{"cell_type":"code","source":"### Your Code Here\n\nclass CustomCallback(TrainerCallback):\n\n    def __init__(self, patience=5):\n        self.patience = patience\n        self.best_loss = float('inf')\n        self.no_improve_num = 0\n\n    def on_train_begin(self, args, state, control, **kwargs):\n        print(\"Training started. Patience set to:\", self.patience)\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs is None:\n            return\n            \n        if 'loss' in logs:\n            loss = logs['loss']\n            \n            ### Preplexity logging \n            perplexity = np.exp(loss)\n            logs[\"perplexity\"] = perplexity\n    \n            ### Checking for early stopping based on patience and tracking best loss\n            if loss < self.best_loss:\n                self.best_loss = loss\n                self.no_improve_num = 0\n    \n            else:\n                self.no_improve_num += 1\n                if self.no_improve_num >= self.patience:\n                    control.should_training_stop = True\n                    print(\"Patience limit reached. Stopping training early.\")\n\n        return control","metadata":{"execution":{"iopub.status.busy":"2025-03-20T17:17:43.534693Z","iopub.execute_input":"2025-03-20T17:17:43.535076Z","iopub.status.idle":"2025-03-20T17:17:43.540706Z","shell.execute_reply.started":"2025-03-20T17:17:43.535049Z","shell.execute_reply":"2025-03-20T17:17:43.539968Z"},"id":"qSuqk1iT8MdE","trusted":true},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"#### Q2.4: TrainingArgs (7 pts)","metadata":{"id":"b4jDQHHI8MdE"}},{"cell_type":"markdown","source":"b. Explain the purpose of a minimum of 5 of the following TrainingArguments parameters in ```at most two sentences.```\nFor each parameter, suggest a good value for our emotion classification problem,\nconsidering we are using a Llama-3.2-1B model and training in a Colab/Kaggle environment.\nExplain why you chose that value.\n\n1.  `lr_scheduler_type`\n2.  `per_device_train_batch_size`\n3.  `gradient_accumulation_steps`\n4.  `learning_rate`\n5.  `weight_decay`\n6.  `bf16`\n7.  `max_grad_norm`\n8.  `warmup_ratio`\n9.  `group_by_length`","metadata":{"id":"t9z_s_di8MdE"}},{"cell_type":"markdown","source":"1.  `lr_scheduler_type` : Determines the schedule used for changing the learning rate during training. A possible good value could be the `linear` or `cosine`. These are fairly common options and therefore are no brainers.\n2.  `per_device_train_batch_size` : Determines number of training samples processed per device (e.g., GPU here) in each step. Given that we are using T4 / P100 GPUs from Google Colaboratory or Kaggle (moderate GPUs), it is best to not make this a large value to overwhelm the device. A value such as 4 or 8 can be good, especially given that we will be using gradient accumulation steps too. It is a small value, and follows the powers of 2 approach in these sorts of hyperparamters.\n3.  `gradient_accumulation_steps` : Allows the training to accumulate gradient over multiple mini-batches (steps) before performing a weight update, hence simulating a larger batch size without extra GPU usage. We can take this to be 8 or 4 to simulate an overall batch size of 32 which is a pretty common batch size in machine learning.\n4.  `learning_rate` : Sets the initial step size used in learning for each parameter update. A value such as `2e-5` is good from what I have searched on the internet.\n5.  `weight_decay` : Determines the penalty associated with L2 regularization for preventing overfitting. Generally, `0.01` is used.\n6.  `bf16` : Makes the training use mixed-precision training which uses memory usage and speeds up training on GPUs that support it. We set it to `True` if it is supported. I believe Tesla T4 supports it but P100 does not.\n7.  `max_grad_norm` : Determines the maximum gradient value for clipping to avoid exploding gradients. 1 is a typical value.\n8.  `warmup_ratio` : Determines ratio of total training spent on increasing the learning rate for an initial warmup, a good technique in training neural networks in general. 0.1 is the typical value here.\n9.  `group_by_length` : Determines whether to use a grouping on sequences of similar length if possible to lessen using padding. Here `True` would probably benefit the efficiency, so we choose it.","metadata":{"id":"-WCOZSury8Nb"}},{"cell_type":"markdown","source":"b. Define your trainings args","metadata":{"id":"f7mjEtzp8MdE"}},{"cell_type":"code","source":"### Your Code Here\n\n# Training arguments\ntrain_args = TrainingArguments(           \n    output_dir=\"./emotion_model/run_1\",\n    num_train_epochs=1,                 \n    per_device_train_batch_size=2,      \n    gradient_accumulation_steps=16,      \n    learning_rate=2e-5,                 \n    weight_decay=0.01,                  \n    lr_scheduler_type=\"linear\",         \n    warmup_ratio=0.1,                   \n    fp16=True,                          \n    max_grad_norm=1.0,   \n    gradient_checkpointing=False,\n    disable_tqdm=False,\n    group_by_length=True,               \n)","metadata":{"execution":{"iopub.status.busy":"2025-03-20T17:37:23.139151Z","iopub.execute_input":"2025-03-20T17:37:23.139456Z","iopub.status.idle":"2025-03-20T17:37:23.175200Z","shell.execute_reply.started":"2025-03-20T17:37:23.139432Z","shell.execute_reply":"2025-03-20T17:37:23.174537Z"},"id":"xKEXnRnV8MdE","trusted":true},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":"#### Q2.5: Memory usage (8 points)","metadata":{"id":"oesSgQLnleqH"}},{"cell_type":"markdown","source":"Now, we want to determine the memory required to **load and train** the LLM in different fine-tuning scenarios.  \n\n- **Full Fine-Tuning:** Calculate the total memory needed when updating all model parameters.  \n- **LoRA Fine-Tuning:** Calculate the memory needed based on your LoRA configuration.  \n- Use your current settings for the calculations.  \n- Refer to [this resource](https://blog.eleuther.ai/transformer-math/) for guidance.","metadata":{"id":"fj_Fk88wlvbN"}},{"cell_type":"markdown","source":"We will look at mixed precision settings using adamw and no activation checkpointing and assuming $t = 1$. Also when tokenizing, max_length was set to 512. This means that the `s` will be 512.\n\nHere are the formulas used in the blog post:\n\n### For inference:\nTotal Memory $_{\\text {Inference }} \\approx(1.2) \\times (2 \\text{bytes}/\\text{param}) \\times (No. \\text{params})$ \n\n### For training:\nThe formula is:\n\n$\\text{Total Memory}_{\\text {Training}}= (2 \\text { bytes } / \\text { param }) \\times(\\text { No. params })+(12 \\text { bytes } / \\text { param }) \\times(\\text { No. params })+(2 \\text { bytes } / \\text { param }) \\times(\\text { No. params })+s b h L\\left(10+\\frac{24}{t}+5 \\frac{a \\cdot s}{h \\cdot t}\\right)$\n\n### Full Fine-Tuning:\nThe number of parameters is 1235814400. Using the following code and the formulas above this comes up to about 27.665 GBs.\n\n### LoRA:\nThe number of parameters is 22544384. Using the following code and the formulas above this comes up to about 9.586 GBs.\n\n","metadata":{"id":"QVmS5BKyoj0p"}},{"cell_type":"code","source":"def calculate_memory(model, name=\"\"):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n\n    P = trainable_params\n    gb = 2 ** 30\n\n    # Model memory (parameters)\n    m_mem = 2 * P / gb\n\n    # Optimizer memory\n    o_mem = 12 * P / gb\n\n    # Gradient memory\n    g_mem = 2 * P / gb\n\n    # Activation memory\n    L = model.config.num_hidden_layers   \n    h = model.config.hidden_size         \n    a = model.config.num_attention_heads  \n    s = 256\n    b = train_args.per_device_train_batch_size\n    t = 1\n    a_mem = (s * b * h * L * (10 + 24 / t + 5 * ((a * s)/(h * t)))) / gb\n    print(f\"\\n[*] {name}:\")\n    print(f\"Parameter memory = {m_mem:.3f} GBs\")\n    print(f\"Optimizer memory = {o_mem:.3f} GBs\")\n    print(f\"Gradient memory = {g_mem:.3f} GBs\")\n    print(f\"Activation memory = {a_mem:.3f} GBs\")\n    mem = m_mem + o_mem + g_mem + a_mem\n    print(f\"Total memory for training = {mem:.3f} GBs\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n        BASE_MODEL,\n        device_map=\"cpu\",\n    )\ncalculate_memory(model, name=\"Full Fine-Tuning\")\ncalculate_memory(lora_model, name=\"LORA\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T17:18:07.312273Z","iopub.execute_input":"2025-03-20T17:18:07.312582Z","iopub.status.idle":"2025-03-20T17:18:09.542789Z","shell.execute_reply.started":"2025-03-20T17:18:07.312538Z","shell.execute_reply":"2025-03-20T17:18:09.542010Z"}},"outputs":[{"name":"stdout","text":"\n[*] Full Fine-Tuning:\nParameter memory = 2.302 GBs\nOptimizer memory = 13.811 GBs\nGradient memory = 2.302 GBs\nActivation memory = 0.844 GBs\nTotal memory for training = 19.259 GBs\n\n[*] LORA:\nParameter memory = 0.001 GBs\nOptimizer memory = 0.005 GBs\nGradient memory = 0.001 GBs\nActivation memory = 0.844 GBs\nTotal memory for training = 0.850 GBs\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"#### Q2.6: Training the model (2 pts)","metadata":{"id":"YYso3Xrn8MdF"}},{"cell_type":"markdown","source":"Train and save the model. Your training should take at most 10 minutes on a Google colab notebook.\n\n***PRO-TIP:*** If you want to go a step further on a good training task, you may research and use model checkpointing and monitoring tools (like weights and biases and tensorboard) But it's not required here.","metadata":{"id":"qV_wnGtE8MdF"}},{"cell_type":"code","source":"### Your Code Here\n\nfrom transformers import DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False  \n)\n\n!pip install wandb --quiet\nimport wandb\nwandb.init(mode=\"disabled\")\n# tokenized_dataset = tokenized_dataset.remove_columns([\"text\", \"label\"])\n\ntrainer = Trainer(\n    model=lora_model,\n    args=train_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    processing_class=tokenizer,\n    data_collator = data_collator,\n    callbacks=[CustomCallback(patience=5)]\n)\n\ntrainer.train()\ntrainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2025-03-20T17:37:27.646927Z","iopub.execute_input":"2025-03-20T17:37:27.647219Z","iopub.status.idle":"2025-03-20T17:55:57.697480Z","shell.execute_reply.started":"2025-03-20T17:37:27.647197Z","shell.execute_reply":"2025-03-20T17:55:57.696693Z"},"id":"b_jJc05S8MdF","trusted":true},"outputs":[{"name":"stdout","text":"Training started. Patience set to: 5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='46' max='46' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [46/46 17:29, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13/13 00:31]\n    </div>\n    "},"metadata":{}},{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 6.511713981628418,\n 'eval_runtime': 32.679,\n 'eval_samples_per_second': 3.06,\n 'eval_steps_per_second': 0.398,\n 'epoch': 0.9813333333333333}"},"metadata":{}}],"execution_count":57},{"cell_type":"markdown","source":"### C. Some other PEFT methods (6 pts)","metadata":{"id":"vyY2GI_JTw3r"}},{"cell_type":"markdown","source":"#### Q2.7: IA3 method (2 points)","metadata":{"id":"HosLXDArT7MT"}},{"cell_type":"markdown","source":"IA3 ([Liu et al., 2022](https://openreview.net/pdf?id=rBCvMG-JsPd)) is another PEFT method. Briefly explain how it works.","metadata":{"id":"fz5hBZTedXc2"}},{"cell_type":"markdown","source":"IA3, Infused Adapter by Inhibiting and Amplifying Inner Activations, is another paramter-efficient fine-tuning method. IA3 adjusts the activations in the model without changing the original weights. IA3 adds learnable scaling vectors into the attention mechanism and feedforward layers of the model. These learnable vectors then can amplify or inhibit the hidden activations. The formula for it in the attention mechanism is:\n$$\\operatorname{softmax}\\left(\\frac{Q\\left(l_{\\mathrm{k}} \\odot K^T\\right)}{\\sqrt{d_k}}\\right)\\left(l_{\\mathrm{v}} \\odot V\\right)$$\n\nand in the feedforwrd layers it is:\n$$\\left(l_{f f} \\odot \\gamma\\left(W_1 x\\right)\\right) W_2$$\n\nwhere $(l_k, l_v, l_{ff})$ are a set of vectors added to each layer and $$\\gamma$$ is the non-linearity.","metadata":{"id":"4oi-m6r2c9kW"}},{"cell_type":"markdown","source":"#### Q2.8: Soft Prompt methods (4 points)","metadata":{"id":"HUnIbezUr8Qo"}},{"cell_type":"markdown","source":"Instead of fine-tuning all model parameters, prompting uses additional input text to guide a frozen model toward a specific task.  \n\nThere are two types of prompts [(Hugging Face, PEFT)](https://huggingface.co/docs/peft/en/conceptual_guides/prompting):  \n- **Hard prompts**: Manually crafted text prompts using discrete tokens, but designing them is labor-intensive.  \n- **Soft prompts**: Learnable tensors concatenated with input embeddings and optimized for a dataset, but they are not human-readable.  \n\nIn this section, you will explore how soft prompts are implemented and fine-tuned using PEFT.\n","metadata":{"id":"ZEoBfkeNz0hd"}},{"cell_type":"markdown","source":"Briefly explain the following soft prompt methods and highlight their key differences:  \n- **Prompt Tuning** [(Lester et al., 2021)](https://aclanthology.org/2021.emnlp-main.243.pdf)  \n- **Prefix Tuning** [(Li & Liang, 2021)](https://aclanthology.org/2021.acl-long.353.pdf)  \n- **P-Tuning** [(Liu et al., 2021)](https://arxiv.org/pdf/2103.10385)  ","metadata":{"id":"8csZKx8is7y-"}},{"cell_type":"markdown","source":"```Prompt Tuning:``` Prompt tuning, another PEFT method, keeps the pre-trained model frozen and adds a small set of task-specific \"soft prompts\" that are trainable embeddings prepended to the input. These tokens are optimized during training to guide the model toward performing desired task.\n\n```Prefix Tuning:``` Prefix tuning optimizes a small set of additional “prefix” vectors (often less than 1% of the full model size) prepended to the input. These trainable prefix embeddings function like “virtual tokens” that push the frozen model to performing well in the desired task.\n\n```P-Tuning:``` Uses a prompt encoder (e.g., a bidirectional LSTM) to generate continuous soft prompts, which are inserted into the input embeddings. These prompts are context-aware (because they are modeled using the encoder).\n\n\n\n","metadata":{"id":"uNEv3hjgiJJC"}},{"cell_type":"markdown","source":"### D. Evaluate and Comparison (24 pts)","metadata":{"id":"1grzEyrLR70S"}},{"cell_type":"markdown","source":"#### Q2.9: Generating Output from Models (10 pts)","metadata":{"id":"bgHePd4hfDJD"}},{"cell_type":"markdown","source":"Generate the output of models on the task of emotion detection using:\n\n- LoRa fine-tuned Model by you\n- Instruction tuned model by Meta\n- Base model by Meta\n\nYou may use ```Regex``` or simply looking for label names in model outputs to do obtain the classification repots. Looking at the results generated by models can help you greatly to find the best way to parse the output.\n\n***NOTE:*** Your fine-tuned model MUST outperform the base model, but outperforming the instruction tuned model is optional and has extra points. (5 pts)","metadata":{"id":"rv2Fo5R18MdF"}},{"cell_type":"code","source":"### Your Code Here\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nfrom tqdm.auto import tqdm\nimport re\n\n\n\n# Save fine-tuned model\ntrainer.save_model(\"./my_model_directory\")\npath = \"./my_model_directory\"\n\n# Test dataset\ntest_ds = sample_dataset[\"test\"]\n\ndef generate_predictions(model, tokenizer, prompts):\n    predictions = []\n\n    for prompt in tqdm(prompts, desc=f\"Generating model predictions\"):\n        system_instruction = \"Analyze the emotion in the text. Choose from: sadness, joy, \" +\\\n            \"love, anger, fear, surprise. Respond with \\\"The emotion is [label].\\\"\"\n        \n        chatml = [\n            {\"role\": \"system\", \"content\": system_instruction},\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n        \n        # Format input based on template\n        if tokenizer.chat_template is not None:\n            inputs = tokenizer.apply_chat_template(chatml, return_tensors=\"pt\").to(model.device)\n        else:\n            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n            inputs = inputs[\"input_ids\"]\n\n\n        model.eval()\n    \n        # Generate response\n        with torch.no_grad():\n            outputs = model.generate(\n                inputs, \n                max_new_tokens=48,\n            )\n        \n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        predictions.append(response)\n        \n    return predictions\n\ndef extract_regex(output):\n    # Extract emotion using regex\n    pattern = r\"emotion (?:is|in this text is) (sadness|joy|love|anger|fear|surprise)\"\n    match = re.search(pattern, output.lower())\n    \n    if match:\n        return match.group(1)\n    \n    emotions = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n    for emotion in emotions:\n        if emotion in output.lower():\n            return emotion\n            \n    return \"unknown\"\n\ndef evaluate_model(predictions, labels):\n    label_names = dataset[\"train\"].features[\"label\"].names + [\"unknown\"]\n    label_dict = dict(zip(label_names, range(len(label_names))))\n\n    pred_emotions = [extract_regex(output) if extract_regex(output) in label_dict else \"unknown\" for output in predictions]\n    pred_emotions_num = [label_dict[value] for value in pred_emotions]\n\n    # Metrics and report\n    accuracy = accuracy_score(labels, pred_emotions_num)\n    report = classification_report(labels, pred_emotions_num, zero_division=1)\n    cm = confusion_matrix(labels, pred_emotions_num, labels=list(range(len(label_names))))\n\n    return {\n        \"accuracy\": accuracy,\n        \"report\": report,\n        \"confusion_matrix\": cm,\n    }\n\nprint()\ndel model\ndel base_model\ndel lora_model\ntorch.cuda.empty_cache()\nmodel = AutoModelForCausalLM.from_pretrained(BASE_MODEL)\n# Base model evaluation\nbase_model, tokenizer = new_model(model, tokenizer, BASE_MODEL)\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\npredictions = generate_predictions(base_model, tokenizer, test_ds[\"text\"])\nev1 = evaluate_model(predictions, test_ds[\"label\"])\nprint(f\"Base Model Evaluation:\\nAccuracy = {ev1['accuracy']}\\n{ev1['report']}\\n{ev1['confusion_matrix']}\\n\")\n\n\n# Our Fine-tuned model evaluation with LoRA\ntokenizer = AutoTokenizer.from_pretrained(INSTRUCT_MODEL)\nlora_model = PeftModel.from_pretrained(base_model, path)\npredictions = generate_predictions(lora_model, tokenizer, test_ds[\"text\"])\nev2 = evaluate_model(predictions, test_ds[\"label\"])\nprint(f\"LoRA Model Evaluation:\\nAccuracy = {ev2['accuracy']}\\n{ev2['report']}\\n{ev2['confusion_matrix']}\\n\")\n\n\n# Instruct model evaluation\ninstruct_model, tokenizer = new_model(lora_model, tokenizer, INSTRUCT_MODEL)\ntokenizer = AutoTokenizer.from_pretrained(INSTRUCT_MODEL)\npredictions = generate_predictions(instruct_model, tokenizer, test_ds[\"text\"])\nev3 = evaluate_model(predictions, test_ds[\"label\"])\nprint(f\"Instruct Model Evaluation:\\nAccuracy = {ev3['accuracy']}\\n{ev3['report']}\\n{ev3['confusion_matrix']}\\n\")\n","metadata":{"id":"PM6Q7gg90Frt","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T20:04:25.362651Z","iopub.execute_input":"2025-03-20T20:04:25.363021Z","iopub.status.idle":"2025-03-20T20:09:28.588385Z","shell.execute_reply.started":"2025-03-20T20:04:25.362991Z","shell.execute_reply":"2025-03-20T20:09:28.587513Z"}},"outputs":[{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating model predictions:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22355463fa9c4e528d8efeb4e7803546"}},"metadata":{}},{"name":"stdout","text":"Base Model Evaluation:\nAccuracy = 0.05\n              precision    recall  f1-score   support\n\n           0       1.00      0.00      0.00        29\n           1       0.50      0.03      0.05        35\n           2       0.18      0.38      0.24         8\n           3       1.00      0.00      0.00        14\n           4       1.00      0.00      0.00        11\n           5       1.00      0.33      0.50         3\n           6       0.00      1.00      0.00         0\n\n    accuracy                           0.05       100\n   macro avg       0.67      0.25      0.11       100\nweighted avg       0.76      0.05      0.05       100\n\n[[ 0  0  3  0  0  0 26]\n [ 0  1 10  0  0  0 24]\n [ 0  0  3  0  0  0  5]\n [ 0  0  1  0  0  0 13]\n [ 0  1  0  0  0  0 10]\n [ 0  0  0  0  0  1  2]\n [ 0  0  0  0  0  0  0]]\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating model predictions:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3616b7982ff43a4b83a649840bcc007"}},"metadata":{}},{"name":"stdout","text":"LoRA Model Evaluation:\nAccuracy = 0.31\n              precision    recall  f1-score   support\n\n           0       0.30      0.93      0.46        29\n           1       0.44      0.11      0.18        35\n           2       0.00      0.00      0.00         8\n           3       1.00      0.00      0.00        14\n           4       1.00      0.00      0.00        11\n           5       1.00      0.00      0.00         3\n\n    accuracy                           0.31       100\n   macro avg       0.62      0.17      0.11       100\nweighted avg       0.52      0.31      0.20       100\n\n[[27  2  0  0  0  0  0]\n [29  4  2  0  0  0  0]\n [ 7  1  0  0  0  0  0]\n [13  1  0  0  0  0  0]\n [10  1  0  0  0  0  0]\n [ 3  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0]]\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating model predictions:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e9265df692947318a34789782fb16f9"}},"metadata":{}},{"name":"stdout","text":"Instruct Model Evaluation:\nAccuracy = 0.52\n              precision    recall  f1-score   support\n\n           0       0.50      0.93      0.65        29\n           1       0.73      0.46      0.56        35\n           2       0.14      0.12      0.13         8\n           3       0.83      0.36      0.50        14\n           4       0.22      0.18      0.20        11\n           5       0.50      0.33      0.40         3\n\n    accuracy                           0.52       100\n   macro avg       0.49      0.40      0.41       100\nweighted avg       0.57      0.52      0.50       100\n\n[[27  1  0  0  1  0  0]\n [11 16  4  0  3  1  0]\n [ 1  4  1  1  1  0  0]\n [ 8  0  0  5  1  0  0]\n [ 6  1  2  0  2  0  0]\n [ 1  0  0  0  1  1  0]\n [ 0  0  0  0  0  0  0]]\n\n","output_type":"stream"}],"execution_count":107},{"cell_type":"markdown","source":"#### Q2.10: Performance Comparison Visualization (4 pts)","metadata":{"id":"8aIqYpn0fDJE"}},{"cell_type":"markdown","source":"Compare the Accuracy and Micro-F1 in a grouped bar chart. (4 pts)","metadata":{"id":"vIsVZU3Q0Hq7"}},{"cell_type":"code","source":"### Your Code Here\n\ndef plot_accuracy_f1_from_cm(confusion_matrices, model_names, save_path=None):\n    \"\"\"\n    Calculate Accuracy and Micro-F1 from confusion matrices and create a grouped bar chart.\n    \n    Parameters:\n    -----------\n    confusion_matrices : list of numpy.ndarray\n        List of confusion matrices for each model\n    model_names : list of str\n        Names of the models to display\n    save_path : str, optional\n        Path to save the figure\n    \"\"\"\n    accuracies = []\n    micro_f1s = []\n    \n    # Calculate metrics for each confusion matrix\n    for cm in confusion_matrices:\n        # Calculate accuracy\n        accuracy = np.sum(np.diag(cm)) / np.sum(cm)\n        \n        # Calculate micro-F1\n        tp_sum = np.sum(np.diag(cm))\n        fp_sum = np.sum(cm.sum(axis=0) - np.diag(cm))\n        fn_sum = np.sum(cm.sum(axis=1) - np.diag(cm))\n        \n        precision = tp_sum / (tp_sum + fp_sum) if (tp_sum + fp_sum) > 0 else 0\n        recall = tp_sum / (tp_sum + fn_sum) if (tp_sum + fn_sum) > 0 else 0\n        micro_f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n        \n        accuracies.append(accuracy)\n        micro_f1s.append(micro_f1)\n    \n    # Create plot\n    x = np.arange(len(model_names))\n    width = 0.35\n    \n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(x - width/2, accuracies, width, label='Accuracy', color='#8884d8')\n    ax.bar(x + width/2, micro_f1s, width, label='Micro-F1', color='#82ca9d')\n    \n    # Add formatting\n    ax.set_xlabel('Models')\n    ax.set_ylabel('Score')\n    ax.set_title('Accuracy vs Micro-F1 Comparison')\n    ax.set_xticks(x)\n    ax.set_xticklabels(model_names)\n    ax.set_ylim(0, 1.0)\n    ax.legend()\n    \n    # Add value labels\n    for i, v in enumerate(accuracies):\n        ax.text(i - width/2, v + 0.01, f'{v:.3f}', ha='center')\n    for i, v in enumerate(micro_f1s):\n        ax.text(i + width/2, v + 0.01, f'{v:.3f}', ha='center')\n    \n    plt.tight_layout()\n    if save_path:\n        plt.savefig(save_path)\n    plt.show()\n    \n    # Print results\n    print(\"Results:\")\n    for i, model in enumerate(model_names):\n        print(f\"{model}: Accuracy = {accuracies[i]:.3f}, Micro-F1 = {micro_f1s[i]:.3f}\")\n\nplot_accuracy_f1_from_cm(\n        [ev1['confusion_matrix'], ev2['confusion_matrix'], ev3['confusion_matrix']],\n        ['Base Model', 'LoRA Model', 'Instruct Model'],\n        save_path='model_comparison.png'\n    )","metadata":{"id":"dMNV7_180RAC","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T20:13:07.351542Z","iopub.execute_input":"2025-03-20T20:13:07.351920Z","iopub.status.idle":"2025-03-20T20:13:07.665351Z","shell.execute_reply.started":"2025-03-20T20:13:07.351891Z","shell.execute_reply":"2025-03-20T20:13:07.664666Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"Results:\nBase Model: Accuracy = 0.050, Micro-F1 = 0.050\nLoRA Model: Accuracy = 0.310, Micro-F1 = 0.310\nInstruct Model: Accuracy = 0.520, Micro-F1 = 0.520\n","output_type":"stream"}],"execution_count":108},{"cell_type":"markdown","source":"#### Q2.11: Analysis (10 pts)","metadata":{"id":"A5JR1ckPfDJE"}},{"cell_type":"markdown","source":"Here, the result are evident. Our LoRA fine-tuned model enhances the base model predictive capabilitiy substantially. It results in about 30% accuracy in the test data. The reason it is able to exceed the base model is obvious. It is a fine-tuned base model that is gone through the fine-tuning for the specific purposes of this task. Also it deploys an instruction format for its input to the model which helps it as well. \n\nHowever, out LoRA fine-tuned model is unable to beat the instruct model. This is due to the fact that the instruct model has gone through a more extensive fine-tuning and instructing tuning. Although We could have probably fine-tuned a LoRA model that could beat the instruct model, given enough experiments with different abilations and more training epochs.\n\nOverall, the LoRA fine-tuned model is satisfactory.","metadata":{"id":"MFwho05S0nLv"}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}